## 결정 트리의 앙상블

앙상블(ensemble) - 여러 머신러닝 모델을 연결해서 더 강력한 모델을 생성

랜덤 포레스트와 그레이디언트 부스팅 두 앙상블 모델이 분류와 회귀의 다양한 데이터셋에서 효과적

### 랜덤 포레스트

훈련 데이터에 과대적합되는 결정 트리의 단점을 회피할 수 있는 방법

- 조금씩 다른 여러 결정 트리의 묶음

- 잘 동작하지만 서로 다른 방향으로 과대적합된 여러 트리들을 결합해서 그 결과들을 평균내면 성능이 유지되면서 과대적합이 줄어듬

- 각각의 트리는 타깃 예측을 잘 해야 하고 다른 트리와는 구별되어야 함

- 트리 생성 시 무작위성을 주입

  - **부트스트랩 - 트리를 만들 때 사용하는 데이터 포인트를 무작위로 선택하는 방법**

  > n개의 데이터 포인트 중 무작위로 데이터를 n 횟수만큼 반복 추출 (한 샘플이 여러 번 중복 추출되거나 어떤 데이터 포인트는 중복되거나 누락 될수도 있음)
  >
  > 랜덤 포레스트의 트리가 조금씩 다른 데이터셋을 이용해 만들어지도록 함
  >
  > EX) ['a', 'b', 'c', 'd']를 가지고 부트스트랩 샘플 생성
  >
  > ['b', 'd', 'd', 'c'], ['d', 'a', 'd', 'a'], ['a', 'b', 'd', 'c']...

  - **max_feature - 분할 테스트에서 특성을 무작위로 선택하는 방법**

  > 각 노드에서 전체 특성을 대상으로 최선의 테스트를 찾는 것이 아니고 알고리즘이 **각 노드에서 후보 특성을 무작위로 선택한 후 이 중에서 최선의 테스트**를 찾음 
  >
  > max_feature를 전체 피처수로 설정하면 트리의 각 분기에서 모든 특성을 고려하므로 특성 선택에 무작위성X
  >
  > max_feature를 1로 하면 무작위로 선택한 특성의 임계값만 찾으면 됨
  >
  > **max_feature를 크게 하면 랜덤 포레스트의 트리들을 매우 비슷해지고 가장 두드러진 특성을 이용해 데이터에 맞춰짐**
  >
  > **max_feature를 작게 하면 랜덤 포레스트의 트리들을 많이 달라지고 각 트리는 데이터에 맞추기 위해 깊이가 깊어짐** 

회귀의 경우는 모든 트리의 예측들을 평균이 최종 예측이 됨

분류의 경우에는 약한 투표 전략을 사용 (각 트리들이 예측한 확률을 평균내고 가장 높은 확률을 가진 클래스가 최종 예측값)

```python
import mglearn
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split

X, y = make_moons(n_samples=100, noise=0.25, random_state=3)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

forest = RandomForestClassifier(n_estimators=5, random_state=2)
forest.fit(X_train, y_train)

plt.rc('font', family="Malgun Gothic")
fig, axes = plt.subplots(2, 3, figsize=(20, 10))
for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):
    ax.set_title("트리 {}".format(i))
    mglearn.plots.plot_tree_partition(X, y, tree, ax=ax)

mglearn.plots.plot_2d_separator(forest, X, fill=True, ax=axes[-1, -1], alpha=.4)
axes[-1, -1].set_title("랜덤 포레스트")
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
```

<img src="https://user-images.githubusercontent.com/58063806/110198113-7d280880-7e93-11eb-9f02-9dcc75423d51.png" width=100% />

각 트리에서 만들어진 결정 경계들이 다른 것과 랜덤 포레스트는 각각의 트리보다는 덜 과대적합되고 더 좋은 결정 경계를 생성하는 것을 볼 수 있음

```python
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)
forest = RandomForestClassifier(n_estimators=100, random_state=0)
forest.fit(X_train, y_train)

print("train set score: {:.3f}".format(forest.score(X_train, y_train)))
print("test set score: {:.3f}".format(forest.score(X_test, y_test)))

# train set score: 1.000
# test set score: 0.972
```

매개변수 튜닝 없이도 단일 결정 트리보다 높은 97%의 정확도를 냄

```python
import matplotlib.pyplot as plt
import numpy as np 

def plot_feature_importances_cancer(model):
    n_features = cancer.data.shape[1]
    plt.barh(np.arange(n_features), model.feature_importances_, align="center")
    plt.yticks(np.arange(n_features), cancer.feature_names)
    plt.xlabel('feature_importance')
    plt.ylabel('feature')
    plt.ylim(-1, n_features)
    
plot_feature_importances_cancer(forest)
```

<img src="https://user-images.githubusercontent.com/58063806/110198378-64205700-7e95-11eb-896b-9c54e2bd43f7.png" width=70% />

단일 트리의 경우보다 훨씬 많은 특성이 0이상의 중요도를 가짐 (랜덤 포레스트를 만드는 무작위성은 알고리즘이 가능성 있는 많은 경우를 고려할 수 있도록 함)

#### 장단점과 매개변수

- 단일 트리의 단점을 보완하고 장점은 그대로 가짐
- 매개변수 튜닝을 많이 하지않아도 잘 작동
- 데이터 스케일링 필요X
- random_state 값에 따라서 전혀 다른 모델이 생성됨 (트리가 많을 수록 변동이 적음)
- 텍스트 데이터 같이 매우 차원이 높고 희소한 데이터에는 잘 작동하지 않음 (선형 모델이 더 적합)
- 선형 모델보다 많은 메모리를 사용하며 훈련과 예측이 느림
- n_estimator가 클수록 과대적합을 줄이고 더 안정적인 모델이 생성되지만 더 많은 메모리와 훈련 시간이 걸림
- max_feature는 각 트리가 얼마나 무작위가 될지를 결정하며 작은 max_feature는 과대적합을 줄임 (일반적으로 default값 사용하는 것이 좋음)
  - 분류 max_feature = sqrt(n_features)
  - 회귀 max_feature = n_features



### 그래이디언트 부스팅 회귀 트리

- 회귀와 분류 모두에 사용 가능
- 랜덤 포레스트와 달리 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 생성 (무작위성 X, 강력한 사전 가지치기 사용)
  - 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습 
  - 최종적으로는 모든 예측기의 예측을 더함
- 보통 하나에서 다섯 정도의 깊지 않은 트리를 사용 (메모리 적게 사용, 예측이 빠름)
- 간단한 모델(약한 학습기)를 많이 연결하는 것이 근본적인 아이디어
- learning_rate - 이전 트리의 오차를 얼마나 강하게 보정할 것인가를 제어 (학습률이 크면 트리는 보정을 강하게 해서 더 복잡한 모델을 만듬)

```python
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)

gbrt = GradientBoostingClassifier(random_state=0)
gbrt.fit(X_train, y_train)

print("train set score: {:.3f}".format(gbrt.score(X_train, y_train)))
print("test set score: {:.3f}".format(gbrt.score(X_test, y_test)))

# train set score: 1.000
# test set score: 0.965
```

train set score이 100% 이므로 과대적합

**학습률 조정, 사전 가지치기**

```python
gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)
gbrt.fit(X_train, y_train)

print("train set score: {:.3f}".format(gbrt.score(X_train, y_train)))
print("test set score: {:.3f}".format(gbrt.score(X_test, y_test)))

# train set score: 0.991
# test set score: 0.972

gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)
gbrt.fit(X_train, y_train)

print("train set score: {:.3f}".format(gbrt.score(X_train, y_train)))
print("test set score: {:.3f}".format(gbrt.score(X_test, y_test)))

# train set score: 0.988
# test set score: 0.965
```

모델의 복잡도를 감소시켜서 train set score는 감소했지만 max_depth를 낮추는 것은 test set score를 개선시킴

```python
gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)
gbrt.fit(X_train, y_train)

plot_feature_importances_cancer(gbrt)
```

<img src="https://user-images.githubusercontent.com/58063806/110230924-b5911a80-7f57-11eb-9546-f3277618b4c0.png" width=70% />

랜덤 포레스트와 비슷한 특성을 강조하지만 일부 특성을 완전히 무시하는 것을 볼 수 있음

> 비슷한 종류의 데이터에서 그레이디언트 부스팅과 랜덤 포레스트 둘 다 잘 작동하지만, 보통 더 안정적인 랜덤 포레스트를 먼저 적용
>
> 랜덤 포레스트가 잘 작동하더라도 예측 시간이 중요하거나 마지막 성능까지 쥐어짜야 할 때 그레이디언트 부스팅을 사용하는 것이 도움이 됨

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_boston

X, y = load_boston().data, load_boston().target
X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=10)

gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=500, random_state=10)
gbrt.fit(X_train, y_train)

# 각 훈련 단계에서 검증 오차를 측정
errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]
bst_n_estimators = np.argmin(errors) + 1
print(bst_n_estimators)
# 206

gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=10)
gbrt_best.fit(X_train, y_train)

print("fixed n_estimators {:.3f}".format(gbrt.score(X_val, y_val)))
print("optimized n_estimators {:.3f}".format(gbrt_best.score(X_val, y_val)))

# fixed n_estimators 0.865
# optimized n_estimators 0.868
```

- 지정된 모든 트리를 이용해 학습시킨 경우보다 staged_predict()를 이용해 최적의 트리 수를 찾고 이를 이용해 학습시킨 경우에 성능이 미세하지만 더 좋았음
  - staged_predict() : 훈련의 각 단계에서 앙상블에 의해 만들어진 예측기를 순회하는  반복자를 반환

#### 장단점과 매개변수

- 매개변수를 잘 조정해야하고 훈련 시간이 김
- 특성의 스케일을 조정하지 않아도 되며 이진 특성이나 연속적인 특성에서도 잘 작동
- 희소한 고차원 데이터에는 잘 작동X (트리 기반 모델의 특성)
- n_estimators가 클수록 좋은 랜덤 포레스트와는 달리 그레이디언트 부스팅에서는 n_estimators를 크게 하면 모델이 복잡해지고 과대적합될 가능성이 높아짐
- 가용한 시간과 메모리 한도에서 **n_estimators를 맞추고나서 적절한 learning_rate를 찾는 것**이 일반적인 관례
- 통상 그레이디언트 부스팅 모델에서는 max_depth를 매우 작게 설정



### 배깅

- Bootstrap aggregating
- 중복을 허용한 랜덤 샘플링으로 만든 훈련 세트를 사용해서 분류기를 각기 다르게 학습
- 분류기가 predict_proba() 메서드를 지원하면 확률값을 평균해서 예측을 수행하고 그렇지 않으면 가장 빈도가 높은 클래스 레이블을 예측 결과로 함

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)
```

```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import BaggingClassifier

bagging = BaggingClassifier(LogisticRegression(), n_estimators=100, oob_score=True, n_jobs=-1, random_state=42)
bagging.fit(X_train, y_train)

print("train set score: {:.3f}".format(bagging.score(X_train, y_train)))
print("test set score: {:.3f}".format(bagging.score(X_test, y_test)))
print("OOB samples score: {:.3f}".format(bagging.oob_score_))

# train set score: 0.962
# test set score: 0.958
# OOB samples score: 0.948
```

- **oob_score를 True로 지정하면 매개변수는 bootstrapping에 포함되지 않은 샘플을 기반으로 훈련된 모델을 평가**함 (out-of-bag score, defalut = False) 
  - 별도의 검증 세트를 사용하지 않고 oob 샘플을 사용해 평가 가능

```python
X, y = make_moons(n_samples=100, noise=0.25, random_state=3)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

from sklearn.tree import DecisionTreeClassifier

bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=5, n_jobs=-1, random_state=42)
bagging.fit(X_train, y_train)

fig, axes = plt.subplots(2, 3, figsize=(20, 10))
for i, (ax, tree) in enumerate(zip(axes.ravel(), bagging.estimators_)):
    ax.set_title("트리 {}".format(i))
    mglearn.plots.plot_tree_partition(X, y, tree, ax=ax)
    
mglearn.plots.plot_2d_separator(bagging, X, fill=True, ax=axes[-1, -1], alpha=.4)
axes[-1, -1].set_title("배깅")
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.show()
```

<img src="https://user-images.githubusercontent.com/58063806/110233668-bed6b300-7f68-11eb-9569-0e6b3387d846.png" width=100% />

```python
cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)

bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, n_jobs=-1, oob_score=True, random_state=42)
bagging.fit(X_train, y_train)

print("train set score: {:.3f}".format(bagging.score(X_train, y_train)))
print("test set score: {:.3f}".format(bagging.score(X_test, y_test)))
print("OOB samples score: {:.3f}".format(bagging.oob_score_))

# train set score: 1.000
# test set score: 0.965
# OOB samples score: 0.951
```

- 배깅은 랜덤 포레스트와 달리 max_samples 매개변수에서 **bootstrap sample의 크기를 지정할 수 있음**



### 엑스트라 트리

- 랜덤 포레스트와(DecisionTreeClassifier(splitter='best'))는 달리 DecisionTreeClassifier(splitter='random')을 사용하고 부트스트랩 샘플링은 적용X
- 특성을 무작위로 분할하고 최적의 임계값을 찾는 대신 후보 특성을 이용해 무작위로 분할한 다음 최적의 분할을 찾음 
- 무작위성을 증가시켜서 일반적으로 모델의 편향이 늘어나지만 분산이 감소
- 예측 방식은 랜덤 포레스트와 동일하게 각 트리가 만든 확률값을 평균

```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons, load_breast_cancer

Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=3)
Xm_train, Xm_test, ym_train, ym_test = train_test_split(Xm, ym, stratify=ym, random_state=42)
cancer = load_breast_cancer()
Xc_train, Xc_test, yc_train, yc_test = train_test_split(cancer.data, cancer.target, random_state=0)

import matplotlib.pyplot as plt
import mglearn
from sklearn.ensemble import ExtraTreesClassifier

plt.rc('font', family="Malgun Gothic")

xtree = ExtraTreesClassifier(n_estimators=5, n_jobs=-1, random_state=0)
xtree.fit(Xm_train, ym_train)

fig, axes = plt.subplots(2, 3, figsize=(20, 10))
for i, (ax, tree) in enumerate(zip(axes.ravel(), xtree.estimators_)):
    ax.set_title("트리 {}".format(i))
    mglearn.plots.plot_tree_partition(Xm, ym, tree, ax=ax)
    
mglearn.plots.plot_2d_separator(xtree, Xm, fill=True, ax=axes[-1, -1], alpha=.4)
axes[-1, -1].set_title("엑스트라 트리")
mglearn.discrete_scatter(Xm[:, 0], Xm[:, 1], ym)
plt.show()
```

<img src="https://user-images.githubusercontent.com/58063806/110231882-bcbb2700-7f5d-11eb-9660-d3680ae3ba9c.png" width=100% />

후보 노드를 랜덤하게 분할한 다음 최적의 분할을 찾기 때문에 개별 트리의 결정 경계가 더 복잡해짐

```python
xtree = ExtraTreesClassifier(n_estimators=100, n_jobs=-1, random_state=0)
xtree.fit(Xc_train, yc_train)

print("train set score: {:.3f}".format(xtree.score(X_train, y_train)))
print("test set score: {:.3f}".format(xtree.score(X_test, y_test)))

# train set score: 1.000
# test set score: 0.972
```

랜덤 포레스트와 거의 같은 성능을 내는 것을 볼 수 있음

> 엑스트라 트리가 랜덤 포레스트보다 계산 비용이 비교적 적지만 무작위 분할 때문에 일반화 성능을 높이려면 종종 많은 트리를 생성해야 하기 때문에 보통 랜덤 포레스트를 사용

```python
plot_feature_importances_cancer(xtree)
```

<img src="https://user-images.githubusercontent.com/58063806/110232694-93e96080-7f62-11eb-9027-7b4f2bfb051d.png" width=70% />

특성 중요도는 비교적 랜덤 포레스트와 비슷하게 나타남



### 에이다부스트

- adaptive boosting
- 그레이디언트 부스팅과 마찬가지로 약한 학습기를 사용
- 그레이디언트 부스팅과 마찬가지로 순차적으로 학습해야 하기 때문에 n_jobs 매개변수 지원X
- 그레이디언트 부스팅과는 달리 **이전의 모델이 잘못 분류한 샘플에 가중치를 높여서 다음 모델을 학습**
- 훈련된 각 모델은 성능에 따라 가중치가 부여됨
- 기본적으로 AdaBoostClassifier는 DecisionTreeClassifier(max_depth=1)을 사용하고  AdaBoostRegressor는 DecisionTreeRegressor(max_depth=3)을 사용하지만 base_estimator 매개변수에서 다른 모델을 지정할 수 있음

```python
from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(n_estimators=5, random_state=42)
ada.fit(Xm_train, ym_train)

fig, axes = plt.subplots(2, 3, figsize=(20, 10))
for i, (ax, tree) in enumerate(zip(axes.ravel(), ada.estimators_)):
    ax.set_title("트리 {}".format(i))
    mglearn.plots.plot_tree_partition(Xm, ym, tree, ax=ax)
    
mglearn.plots.plot_2d_separator(ada, Xm, fill=True, ax=axes[-1, -1], alpha=.4)
axes[-1, -1].set_title("에이다부스트")
mglearn.discrete_scatter(Xm[:, 0], Xm[:, 1], ym)
plt.show()
```

<img src="https://user-images.githubusercontent.com/58063806/110232983-89c86180-7f64-11eb-8c3c-6b2246b85fd5.png" width=100% />

깊이가 1인 결정 트리를 사용하기 때문에 각 트리의 결정 경계가 선 하나로 나타남

```python
ada = AdaBoostClassifier(n_estimators=100, random_state=42)
ada.fit(Xc_train, yc_train)

print("train set score: {:.3f}".format(ada.score(X_train, y_train)))
print("test set score: {:.3f}".format(ada.score(X_test, y_test)))

# train set score: 1.000
# test set score: 0.986
```

매우 얕은 트리를 앙상블했기 때문에 일반화 성능이 조금 향상된 것을 볼 수 있음

```python
plot_feature_importances_cancer(ada)
```

<img src="https://user-images.githubusercontent.com/58063806/110233077-edeb2580-7f64-11eb-9513-f83e5f043ec6.png" width=70% />

특성 중요도에서는 다른 모델에서는 부각되지 않았던 'area error' 특성을 크게 강조하는 것을 볼 수 있음