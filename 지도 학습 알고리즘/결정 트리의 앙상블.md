## 결정 트리의 앙상블

앙상블(ensemble) - 여러 머신러닝 모델을 연결해서 더 강력한 모델을 생성

랜덤 포레스트와 그레이디언트 부스팅 두 앙상블 모델이 분류와 회귀의 다양한 데이터셋에서 효과적

### 랜덤 포레스트

훈련 데이터에 과대적합되는 결정 트리의 단점을 회피할 수 있는 방법

- 조금씩 다른 여러 결정 트리의 묶음

- 잘 동작하지만 서로 다른 방향으로 과대적합된 여러 트리들을 결합해서 그 결과들을 평균내면 성능이 유지되면서 과대적합이 줄어듬

- 각각의 트리는 타깃 예측을 잘 해야 하고 다른 트리와는 구별되어야 함

- 트리 생성 시 무작위성을 주입

  - **부트스트랩 - 트리를 만들 때 사용하는 데이터 포인트를 무작위로 선택하는 방법**

  > n개의 데이터 포인트 중 무작위로 데이터를 n 횟수만큼 반복 추출 (한 샘플이 여러 번 중복 추출되거나 어떤 데이터 포인트는 중복되거나 누락 될수도 있음)
  >
  > 랜덤 포레스트의 트리가 조금씩 다른 데이터셋을 이용해 만들어지도록 함
  >
  > EX) ['a', 'b', 'c', 'd']를 가지고 부트스트랩 샘플 생성
  >
  > ['b', 'd', 'd', 'c'], ['d', 'a', 'd', 'a'], ['a', 'b', 'd', 'c']...

  - **max_feature - 분할 테스트에서 특성을 무작위로 선택하는 방법**

  > 각 노드에서 전체 특성을 대상으로 최선의 테스트를 찾는 것이 아니고 알고리즘이 **각 노드에서 후보 특성을 무작위로 선택한 후 이 중에서 최선의 테스트**를 찾음 
  >
  > max_feature를 전체 피처수로 설정하면 트리의 각 분기에서 모든 특성을 고려하므로 특성 선택에 무작위성X
  >
  > max_feature를 1로 하면 무작위로 선택한 특성의 임계값만 찾으면 됨
  >
  > **max_feature를 크게 하면 랜덤 포레스트의 트리들을 매우 비슷해지고 가장 두드러진 특성을 이용해 데이터에 맞춰짐**
  >
  > **max_feature를 작게 하면 랜덤 포레스트의 트리들을 많이 달라지고 각 트리는 데이터에 맞추기 위해 깊이가 깊어짐** 

회귀의 경우는 모든 트리의 예측들을 평균이 최종 예측이 됨

분류의 경우에는 약한 투표 전략을 사용 (각 트리들이 예측한 확률을 평균내고 가장 높은 확률을 가진 클래스가 최종 예측값)

```python
import mglearn
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split

X, y = make_moons(n_samples=100, noise=0.25, random_state=3)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

forest = RandomForestClassifier(n_estimators=5, random_state=2)
forest.fit(X_train, y_train)

plt.rc('font', family="Malgun Gothic")
fig, axes = plt.subplots(2, 3, figsize=(20, 10))
for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):
    ax.set_title("트리 {}".format(i))
    mglearn.plots.plot_tree_partition(X, y, tree, ax=ax)

mglearn.plots.plot_2d_separator(forest, X, fill=True, ax=axes[-1, -1], alpha=.4)
axes[-1, -1].set_title("랜덤 포레스트")
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
```

<img src="https://user-images.githubusercontent.com/58063806/110198113-7d280880-7e93-11eb-9f02-9dcc75423d51.png" width=100% />

각 트리에서 만들어진 결정 경계들이 다른 것과 랜덤 포레스트는 각각의 트리보다는 덜 과대적합되고 더 좋은 결정 경계를 생성하는 것을 볼 수 있음

```python
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)
forest = RandomForestClassifier(n_estimators=100, random_state=0)
forest.fit(X_train, y_train)

print("train set score: {:.3f}".format(forest.score(X_train, y_train)))
print("test set score: {:.3f}".format(forest.score(X_test, y_test)))

# train set score: 1.000
# test set score: 0.972
```

매개변수 튜닝 없이도 단일 결정 트리보다 높은 97%의 정확도를 냄

```python
import matplotlib.pyplot as plt
import numpy as np 

def plot_feature_importances_cancer(model):
    n_features = cancer.data.shape[1]
    plt.barh(np.arange(n_features), model.feature_importances_, align="center")
    plt.yticks(np.arange(n_features), cancer.feature_names)
    plt.xlabel('feature_importance')
    plt.ylabel('feature')
    plt.ylim(-1, n_features)
    
plot_feature_importances_cancer(forest)
```

<img src="https://user-images.githubusercontent.com/58063806/110198378-64205700-7e95-11eb-896b-9c54e2bd43f7.png" width=70% />

단일 트리의 경우보다 훨씬 많은 특성이 0이상의 중요도를 가짐 (랜덤 포레스트를 만드는 무작위성은 알고리즘이 가능성 있는 많은 경우를 고려할 수 있도록 함)

#### 장단점과 매개변수

- 단일 트리의 단점을 보완하고 장점은 그대로 가짐
- 매개변수 튜닝을 많이 하지않아도 잘 작동
- 데이터 스케일링 필요X
- random_state 값에 따라서 전혀 다른 모델이 생성됨 (트리가 많을 수록 변동이 적음)
- 텍스트 데이터 같이 매우 차원이 높고 희소한 데이터에는 잘 작동하지 않음 (선형 모델이 더 적합)
- 선형 모델보다 많은 메모리를 사용하며 훈련과 예측이 느림
- n_estimator가 클수록 과대적합을 줄이고 더 안정적인 모델이 생성되지만 더 많은 메모리와 훈련 시간이 걸림
- max_feature는 각 트리가 얼마나 무작위가 될지를 결정하며 작은 max_feature는 과대적합을 줄임 (일반적으로 default값 사용하는 것이 좋음)
  - 분류 max_feature = sqrt(n_features)
  - 회귀 max_feature = n_features

