## 선형 모델

- 입력 특성에 대한 선형 함수를 만들어 예측을 수행
- 회귀를 위한 선형 모델은 특성이 하나일 땐 직선, 두 개일 땐 평면, 더 높은 차원(다수의 특성)에서는 초평면이 됨
- 특성이 많은 데이터셋이라면 선형 모델은 훌륭한 성능을 낼 수 있음
- 훈련 데이터로부터 모델 파라미터 w(가중치)와 b(편향)을 학습하는 방법과 모델의 복잡도를 제어하는 방식에 차이를 둔 몇개의 선형 모델들이 있음

### 회귀형 선형 모델

#### 선형 회귀

```python
from sklearn.linear_model import *
import mglearn
from sklearn.model_selection import train_test_split

X, y = mglearn.datasets.make_wave(n_samples=60)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

lr = LinearRegression().fit(X_train, y_train)

print("lr.coef_:", lr.coef_) # 가중치
print("lr.intercept_:", lr.intercept_) # 편향

print("\ntrain set score: {:.2f}".format(lr.score(X_train, y_train)))
print("test set score: {:.2f}".format(lr.score(X_test, y_test)))

# lr.coef_: [0.39390555]
# lr.intercept_: -0.031804343026759746

# train set score: 0.67
# test set score: 0.66
```

train set score와 test set score가 매우 유사 (과소적합의 상태)

```python
from sklearn.linear_model import *
import mglearn
from sklearn.model_selection import train_test_split

X, y = mglearn.datasets.load_extended_boston() # 104개의 특성을 가지는 dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

lr = LinearRegression().fit(X_train, y_train)

print("ntrain set score: {:.2f}".format(lr.score(X_train, y_train)))
print("test set score: {:.2f}".format(lr.score(X_test, y_test)))

# train set score: 0.95
# test set score: 0.61
```

train set score가 test set score에 비해 매우 높은 (과대적합의 상태)

#### 리지 회귀

- 리지 회귀에서의 w(가중치) 선택은 훈련 데이터를 잘 예측하는 것 뿐만 아니라 추가 제약 조건을 만족시키기 위한 목적
- w의 모든 원소가 0에 가깝게 (절댓값을 가능한 한 작게) 만듬
- 모든 특성이 출력에 주는 영향을 최소화 (regularization, 규제 - 과대적합을 방지하기 위해 모델을 강제로 제한)
- **L2 규제를 사용**

```python
ridge = Ridge().fit(X_train, y_train)

print("ntrain set score: {:.2f}".format(ridge.score(X_train, y_train)))
print("test set score: {:.2f}".format(ridge.score(X_test, y_test)))

# train set score: 0.89
# test set score: 0.75

ridge10 = Ridge(alpha=10).fit(X_train, y_train)

# train set score: 0.79
# test set score: 0.64

ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)

# train set score: 0.93
# test set score: 0.77
```

- 선형 회귀에 비해 train set score가 낮아졌지만 test set score는 더 높아짐
- 모델의 복잡도를 낮추어서 일반화된 모델을 생성
- alpha 매개변수로 훈련 세트의 성능 대비 모델을 얼마나 단순화할지 지정가능                                                 (alpha 값을 높이면 계수를 0에 더 가깝게 만들어서 일반화에는 더 도움을 줄 수 있음)

```python
import matplotlib.pyplot as plt 

plt.figure(figsize=(15, 10))
plt.rc('font', family="Malgun Gothic")
plt.rcParams['axes.unicode_minus'] = False
plt.plot(ridge10.coef_, alpha=0.7, label="Ridge alpha=10")
plt.plot(ridge.coef_, alpha=0.7, label="Ridge alpha=1")
plt.plot(ridge01.coef_, alpha=0.7, label="Ridge alpha=0.1")

plt.plot(lr.coef_, alpha=0.5, label="LinearRegression")
plt.xlabel("계수 목록")
plt.ylabel("계수 크기")
xlims = plt.xlim()
plt.hlines(0, xlims[0], xlims[1], color="black")
plt.xlim(xlims)
plt.ylim(-25, 25)
plt.legend()
```

<img src="https://user-images.githubusercontent.com/58063806/109468308-c0f3ca00-7aaf-11eb-9c15-b0b430c91115.png" width=80% />

alpha 값이 커질수록 특성별 가중치 값의 차이가 줄어듬

```python
mglearn.plots.plot_ridge_n_samples()
```

<img src="https://user-images.githubusercontent.com/58063806/109468617-32cc1380-7ab0-11eb-9f4c-35f3a939360b.png" width=50% />

- 모든 데이터셋에 대해 모두 train set score가 더 높음
- 리지에는 규제가 적용되므로 train set score가 전체적으로 선형 회귀의 점수보다 낮음
- test set score는 리지의 점수가 더 높으며 특히 작은 데이터셋에서 그 차이가 극명히 나타남
- 데이터를 충분히 주면 규제 항은 덜 중요해져서 점점 더 리지와 선형 회귀의 성능은 비슷해질 것임

#### 라소 회귀

- 리지 회귀와 마찬가지로 계수를 0에 가깝게 만드는 것을 목표로 함
- **L1 규제를 사용, 어떤 계수는 정말 0이 됨 (특성 선택이 자동으로 이루어짐)**

```python
import numpy as np

lasso = Lasso().fit(X_train, y_train)

print("train set score: {:.2f}".format(lasso.score(X_train, y_train)))
print("test set score: {:.2f}".format(lasso.score(X_test, y_test)))
print("count of used feature:", np.sum(lasso.coef_ != 0))

# train set score: 0.29
# test set score: 0.21
# count of used feature: 4

lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)
# alpha 값을 낮추려면 max_iter를 늘려야함 

# train set score: 0.90
# test set score: 0.77
# count of used feature: 33

lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)

# train set score: 0.95
# test set score: 0.64
# count of used feature: 96
```

- train set score와 test set score 모두 결과가 좋지 않음
- 과소적합, 104개의 특성 중 4개만 사용됨
- alpha 값을 낮추면  모델의 복잡도는 증가하여 train set score와 test set score 모두 성능이 좋아짐 (너무 낮추면 규제의 효과가 없어져 과대적합이 됨)

 ```python
plt.figure(figsize=(15, 10))
plt.rc('font', family="Malgun Gothic")
plt.rcParams['axes.unicode_minus'] = False
plt.plot(lasso.coef_, "s", label="Lasso alpha=1")
plt.plot(lasso001.coef_, "^", label="Lasso alpha=0.01")
plt.plot(lasso00001.coef_, "v", label="Lasso alpha=0.0001")

plt.plot(ridge01.coef_, "o", label="Ridge alpha=0.1")
plt.xlabel("계수 목록")
plt.ylabel("계수 크기")
plt.legend()
plt.ylim(-25, 25)
plt.legend()
 ```

<img src="https://user-images.githubusercontent.com/58063806/109471081-9ad02900-7ab3-11eb-8de7-4e6e8f8addea.png" width=80% />

- lasso 모델에서 alpha 1, 0.01일 때는 많은 특성들의 계수가 0인 것을 볼 수 있음
- Ridge alpha 0.1 모델과 lasso alpha 0.01 모델은 성능이 비슷하지만 계수에는 많은 차이가 있음
- 보통은 리지 회귀를 선호하지만 특성이 많고 그중 일부분만 중요하다면 라소 회귀가 더 좋은 선택
- Lasso와 Ridge의 페널티를 결합한 ElasticNet

### 분류형 선형 모델

- 선형 회귀와 동일한 방식으로 가중치 합을 구한 뒤 이것을 그냥 사용하는 대신 임계치 0과 비교해서 가중합이 0보다 크면 1, 작으면 -1로 예측

```python
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

logistic = LogisticRegression()
svc = LinearSVC()

X,y = mglearn.datasets.make_forge()

fig, axes = plt.subplots(1, 2, figsize=(15, 5))

for model, ax in zip([svc, logistic], axes):
    clf = model.fit(X, y)
    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, ax=ax, alpha=.7)
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
    ax.set_title(clf.__class__.__name__)
    ax.set_xlabel('특성 0')
    ax.set_ylabel('특성 1')
axes[0].legend()
```

<img src="https://user-images.githubusercontent.com/58063806/109473469-a96c0f80-7ab6-11eb-8836-1e9767f79b07.png" width=90%/>

- 위의 두 모델은 기본적으로 L2 규제를 사용
- 규제의 강도를 결정하는 매개변수는 C이며 이 값이 높아지면 규제가 감소함

```python
mglearn.plots.plot_linear_svc_regularization()
```

<img src="https://user-images.githubusercontent.com/58063806/109474151-8c840c00-7ab7-11eb-8402-fe5ff36080a6.png" width=80%/>

- 왼쪽 그림은 작은 C값으로 인해 많은 규제가 적용 (비교적 수평에 가까운 결정 경계를 생성)
- 오른쪽 그림을 보면 규제를 감소함으로써 결정 경계가 기울어지고 클래스 0의 모든 데이터 포인트를 올바르게 분류함

```python
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)
logreg = LogisticRegression().fit(X_train, y_train)

print("train set score: {:.3f}".format(logreg.score(X_train, y_train)))
print("test set score: {:.3f}".format(logreg.score(X_test, y_test)))

# train set score: 0.955
# test set score: 0.958

logreg100 = LogisticRegression(C=100).fit(X_train, y_train)

# train set score: 0.972
# test set score: 0.965

logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)

# train set score: 0.934
# test set score: 0.930
```

- 기본값 C=1에서 train set score, test set score 모두 95% 정도로 좋은 성능을 보이지만 두 점수가 유사한 것으로 보아 과소적합으로 보임
- C=100을 사용해서 제약을 더 풀어주니 train set score가 높아졌고 test set score도 조금 증가함 (복잡도가 높은 모델일수록 성능이 좋음)
- C=0.01을 사용해서 더욱 제약을 높이니 기존의 score들에 비해 더욱 감소

```python
plt.figure(figsize=(12, 8))
plt.rc('font', family="Malgun Gothic")
plt.rcParams['axes.unicode_minus'] = False
plt.plot(logreg100.coef_.T, label="C=100")
plt.plot(logreg.coef_.T, label="C=1")
plt.plot(logreg001.coef_.T, label="C=0.01")

plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)
xlims = plt.xlim()
plt.hlines(0, xlims[0], xlims[1], color="black")
plt.xlim(xlims)
plt.xlabel("특성")
plt.ylabel("계수 크기")
plt.ylim(-5, 5)
plt.legend()
```

<img src="https://user-images.githubusercontent.com/58063806/109476060-cb1ac600-7ab9-11eb-95b5-9597bea7ac88.png" width=80%/>

```python
plt.figure(figsize=(12, 8))
for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):
    lr_l1 = LogisticRegression(solver='liblinear', C=C, penalty="l1").fit(X_train, y_train)
    print("C={:.3f} L1_logistic regression train set score: {:.2f}".format(C, lr_l1.score(X_train, y_train)))
    print("C={:.3f} L1_logistic regression test set score: {:.2f}".format(C, lr_l1.score(X_test, y_test)))
    plt.plot(lr_l1.coef_.T, marker, label="C={:.3f}".format(C))
    
plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)
xlims = plt.xlim()
plt.hlines(0, xlims[0], xlims[1], color="black")
plt.xlim(xlims)
plt.xlabel("특성")
plt.ylabel("계수 크기")
plt.ylim(-5, 5)
plt.legend(loc=3)

# C=0.001 L1_logistic regression train set score: 0.91
# C=0.001 L1_logistic regression test set score: 0.92
# C=1.000 L1_logistic regression train set score: 0.96
# C=1.000 L1_logistic regression test set score: 0.96
# C=100.000 L1_logistic regression train set score: 0.99
# C=100.000 L1_logistic regression test set score: 0.98
```

<img src="https://user-images.githubusercontent.com/58063806/109477672-a889ac80-7abb-11eb-952a-fddb196bfd35.png" width=80% />

- L1 규제를 사용했을 때 대부분의 특성의 계수가 0이 되는 것을 볼 수 있음
- 이진 분류와 회귀에서 선형 모델은 유사점이 많음 (모델들의 주요 차이는 penalty 매개변수) 

#### 다중 클래스 분류용 선형 모델

- 로지스틱 회귀를 제외하고 많은 선형 분류 모델은 이진 분류만을 지원
- 이진 분류 알고리즘을 다중 클래스 분류 알고리즘으로 확장하는 방법은 각 클래스를 다른 모든 클래스와 구분하도록 학습 시키고 결국 클래스 수만큼 이진 분류 모델을 만드는 일대다 방법이 있음 (가장 높은 점수를 내는 분류기의 클래스를 예측값으로 선택)

```python
from sklearn.datasets import make_blobs

X, y = make_blobs(random_state=42)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("특성 0")
plt.ylabel("특성 1")
plt.legend(["클래스 0", "클래스 1", "클래스 2"])
```

<img src="https://user-images.githubusercontent.com/58063806/109489017-a595b880-7ac9-11eb-9b0c-bdec09834b6d.png" width=50% />

```python
linear_svm = LinearSVC().fit(X, y)
print(linear_svm.coef_.shape)
print(linear_svm.intercept_.shape)

# (3, 2) - 세 개의 클래스, 두 개의 특성
# (3, ) - 각 클래스의 편향
```

```python
mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
line = np.linspace(-15, 15)
for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, mglearn.cm3.colors):
    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)
plt.xlabel("특성 0")
plt.ylabel("특성 1")
plt.legend(["클래스 0", "클래스 1", "클래스 2", "클래스 0 경계", "클래스 1 경계", "클래스 2 경계"], loc=(1.01, 0.3))
```

<img src="https://user-images.githubusercontent.com/58063806/109490979-5a30d980-7acc-11eb-97de-e7e738d39a23.png" width=60%/>

#### 장단점과 매개변수

- 선형 모델의 주요 매개변수는 회귀 모델에서는 alpha, LinearSVC, LogisticRegression에서는 C
- alpha 값이 클수록, C 값이 작을수록 모델이 단순해짐
- 선형 모델은 학습 속도가 빠르고 예측도 빠름 (매우 큰 데이터셋과 희소한 데이터셋에서도 잘 작동)
- 수십만에서 수백만 개의 샘플로 이뤄진 대용량 데이터셋이라면 solver="sag" or "saga" 옵션을 줌
- 대용량 처리 버전으로 구현된 SGDClassifier, SGDRegressor 사용
- 데이터셋의 특성들이 서로 깊게 연관되어 있을때 계수를 분석하기 매우 어려울 수 있음
- 선형 모델은 샘플에 비해 특성이 많을 때 잘 작동함

