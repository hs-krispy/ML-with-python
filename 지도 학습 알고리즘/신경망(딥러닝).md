## 신경망(딥러닝)

```python
import mglearn 

display(mglearn.plots.plot_single_hidden_layer_graph())
```

<img src="https://user-images.githubusercontent.com/58063806/111335413-b6dce880-86b7-11eb-9ded-15ceaf2346a2.png" width=30% />

- 선형 모델보다 강력하게 만들기 위해 가중치의 합을 계산하고 비선형 함수인 relu, tanh 등을 적용 

```python
import numpy as np
import matplotlib.pyplot as plt

line = np.linspace(-3, 3, 100)
plt.plot(line, np.tanh(line), label="tanh")
plt.plot(line, np.maximum(line, 0), "--", label="relu")
plt.legend(loc="best")
plt.xlabel("x")
plt.ylabel("relu(x), tanh(x)")
```

<img src="https://user-images.githubusercontent.com/58063806/111335715-f3104900-86b7-11eb-96ff-113feeba1d36.png" width=50% />

```python
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split

plt.rc("font", family="Malgun Gothic")
X, y = make_moons(n_samples=100, noise=0.25, random_state=3)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

mlp = MLPClassifier(solver="lbfgs", random_state=0).fit(X_train, y_train)
mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)
mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)
plt.xlabel("특성 0")
plt.ylabel("특성 1")
```

<img src="https://user-images.githubusercontent.com/58063806/111336873-f3f5aa80-86b8-11eb-8166-00a5ac2d0ddf.png" width=50% />

신경망은 비선형적이지만 비교적 매끄러운 결정 경계를 생성

```python
mlp = MLPClassifier(solver="lbfgs", hidden_layer_sizes=10, random_state=0).fit(X_train, y_train)
mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)
mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)
plt.xlabel("특성 0")
plt.ylabel("특성 1")
```

MLP는 기본적으로 은닉유닛 100개를 사용하는데 해당 데이터셋같은 작은 데이터셋에서는 이 값을 10으로 낮춰도 좋은 결과를 얻음 **(기본 활성화 함수는 relu)**

<img src="https://user-images.githubusercontent.com/58063806/111337117-2901fd00-86b9-11eb-9ef8-7faa69f21e68.png" width=50% />

```python
mlp = MLPClassifier(solver="lbfgs", hidden_layer_sizes=[10, 10], random_state=0).fit(X_train, y_train)
mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)
mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)
plt.xlabel("특성 0")
plt.ylabel("특성 1")
```

은닉층을 하나 더 추가해서 조금 더 매끄러운 결정 경계를 생성 

<img src="https://user-images.githubusercontent.com/58063806/111337851-c65d3100-86b9-11eb-9251-501fbaa792f5.png" width=50% />

```python
fig, axes = plt.subplots(2, 4 ,figsize=(20, 8))
for axx, n_hidden_nodes in zip(axes, [10, 100]):
    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):
        mlp = MLPClassifier(solver="lbfgs", random_state=0, hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes], alpha=alpha)
        mlp.fit(X_train, y_train)
        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)
        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)
        ax.set_title("n_hidden=[{}, {}]\nalpha={:.4f}".format(n_hidden_nodes, n_hidden_nodes, alpha))
```

선형 분류기와 마찬가지로 L2 페널티를 이용해 가중치를 규제할 수 있음

**은닉 유닛의 개수와 L2 페널티에 따른 결정 경계의 변화**

<img src="https://user-images.githubusercontent.com/58063806/111338966-b7c34980-86ba-11eb-91f4-927e7458aa3e.png" width=100% />

```python
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)

mlp = MLPClassifier(random_state=42).fit(X_train, y_train)

print("train set score:{:.2f}".format(mlp.score(X_train, y_train)))
print("test set score:{:.2f}".format(mlp.score(X_test, y_test)))
# train set score: 0.94
# test set score: 0.92
```

모든 입력 특성을 평균 0, 분산 1이 되도록 변형 (StandardScaler)

```python
# 훈련 세트 각 특성의 평균과 표준 편차
mean_on_train = X_train.mean(axis=0)
std_on_train = X_train.std(axis=0)

X_train_scaled = (X_train - mean_on_train) / std_on_train
X_test_scaled = (X_test - mean_on_train) / std_on_train

mlp = MLPClassifier(random_state=0).fit(X_train_scaled, y_train)

print("train set score: {:.2f}".format(mlp.score(X_train_scaled, y_train)))
print("test set score: {:.2f}".format(mlp.score(X_test_scaled, y_test)))

# train set score: 0.99
#test set score: 0.97
```

max_iter로 parameter로 학습 반복횟수를 지정 가능

```python
plt.rcParams['axes.unicode_minus'] = False
plt.figure(figsize=(20, 5))
plt.imshow(mlp.coefs_[0], interpolation="none", cmap="viridis")
plt.yticks(range(30), cancer.feature_names)
plt.xlabel("은닉 유닛")
plt.ylabel("입력 특성")
plt.colorbar()
```

<img src="https://user-images.githubusercontent.com/58063806/111341527-f0fcb900-86bc-11eb-94cb-4d3d90509f67.png" width=100% />

입력층과 은닉층 사이의 학습된 가중치 (모든 은닉 유닛에서 작은 가중치를 가진 특성은 모델에 덜 중요하다고 추론 가능)

#### 장단점과 매개변수

- 대량의 데이터에 내재된 정보를 잡아내고 매우 복잡한 모델을 생성가능
  - 충분한 연산 시간과 데이터를 주고 매개변수를 잘 조정하면 신경망은 다른 머신러닝 알고리즘을 능가하는 성능을 낼 수 있음
- 학습이 오래걸림
- 데이터 전처리에 주의해야함
- solver 매개변수 - 기본값은 adam으로 데이터 스케일에 조금 민감하며 다른 하나는 lbfgs로 안정적이지만 대량의 데이터셋에서는 시간이 오래 걸림

#### 신경망의 복잡도 추정

특성 100개와 은닉 유닛 100개를 가진 이진 분류

입력층과 첫 번째 은닉층 사이에는 편향을 포함해서 100 * 100 + 100 = 10100개의 가중치 

은닉층과 출력층 사이에는 100 * 1 + 1 = 101개의 가중치

총 **10201개의 가중치**

유닛이 100개인 은닉층을 하나 추가하면 10100개의 가중치가 더 늘어남

유닛이 1000개인 은닉층을 하나를 사용하면 100 * 1000 + 1000 = 101000개의 가중치

유닛이 1000개인 은닉층을 두개 사용하면 101000 + 1000 * 1000 + 1000 = 1102000의 가중치

>  신경망의 매개변수를 조정하는 일반적인 방법은 충분히 과대적합되어서 문제를 해결할 만한 큰 모델을 생성하고 신경망 구조를 줄이거나 규제를 강화해서 일반화 성능을 향상 시킴

