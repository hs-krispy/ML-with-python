## 나이브 베이즈 분류기

- 선형 모델과 매우 유사 (선형 분류기보다 훈련 속도가 빠른편이지만 일반화 성능이 조금 뒤짐)
- 각 특성을 개별로 취급해 파라미터를 학습하고 각 특성에서 클래스별 통계를 단순하게 취합하기 때문에 효과적
- GaussianNB : 연속적인 어떤 데이터에도 적용가능
- BernoulliNB : 이진 데이터에 적용
- MultinomialNB : 카운트 데이터 (특성이 어떤 것을 헤아린 정수 카운트, EX) 문장에 나타난 단어의 횟수)
- BernoulliNB, MultinomialNB는 대부분 텍스트 데이터 분류에 사용됨

```python
import numpy as np

# 이진 특성을 4개 가진 데이터 포인트 4개
X = np.array([[0, 1, 0, 1], [1, 0, 1, 1], [0, 0, 0, 1], [1, 0, 1, 0]])
# 클래스는 0, 1 두 개
y = np.array([0, 1, 0, 1])

# 클래스별로 0이 아닌 원소를 카운트
counts = {}
for label in np.unique(y):
    counts[label] = X[y == label].sum(axis=0)
print("feature count:\n", counts)

# feature count:
# {0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}
```

MultinomialNB는 클래스별로 특성의 평균을 계산

GaussianNB는 클래스별로 각 특성의 표준편차와 평균을 저장

예측시에는 데이터 포인트를 클래스의 통계 값과 비교해서 가장 잘 맞는 클래스를 예측값으로 함

#### 장단점과 매개변수

- BernoulliNB, MultinomialNB는 모델의 복잡도를 조절하는 alpha 매개변수 하나를 가짐
  - alpha가 주어지면 모든 특성에 양의 값을 가진 가상의 데이터 포인트를 alpha 개수만큼 추가 (통계 데이터를 완만하게 만들어 줌)
  - alpha가 클수록 더 완만해지고 모델의 복잡도가 낮아짐
  - 하지만 alpha 값이 성능 향상에 크게 기여하지 않음
- GaussianNB는 대부분 매우 고차원 데이터셋에 사용되고 나머지 두 모델은 텍스트 같은 희소한 데이터를 카운트하는 데 사용
- MultinomialNB는 보통 0이 아닌 특성이 비교적 많은 데이터셋( EX) 큰 문서들)에서 BernoulliNB보다 성능이 좋음
- 선형 모델로 학습 시간이 너무 오래 걸리는 매우 큰 데이터셋에서는 나이브 베이즈 모델을 시도해볼 수 있음 