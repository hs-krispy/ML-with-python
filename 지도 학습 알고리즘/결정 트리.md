## 결정 트리

- 결정에 다다르기 위해 가능한 적은 예/아니오 질문을 이어 나가면서 학습 (이러한 질문들은 테스트라고 함)
- 보통 데이터는 예/아니오 형태의 특성으로 구성되기 보다는 연속된 특성으로 구성됨
- 트리를 만들 때 가능한 모든 테스트에서 타깃값에 대해 가장 많은 정보를 가진 것을 고름
- 테스트를 통해 **각 분할된 영역이 한 개의 타깃값(하나의 클래스나 하나의 회귀 분석 결과)을 가질 때까지** 반복 **(타깃 하나로만 이뤄진 리프 노드를 순수 노드라고 함)** 

<img src="https://user-images.githubusercontent.com/58063806/109649077-bf54ff80-7b9e-11eb-9246-f4d76685111b.png" width=45%/>

<img src="https://user-images.githubusercontent.com/58063806/109648432-f4148700-7b9d-11eb-8e52-fefca94264cc.png" width=60% />

#### 결정 트리의 복잡도 제어

- 일반적으로 모든 리프 노드가 순수 노드가 될 때까지 진행하면 모델이 매우 복잡해지고 훈련 데이터에 과대적합됨 
  - EX) 위의 깊이가 9인 경우, 결정 경계가 클래스의 포인트들에서 멀리 떨어진 이상치 하나에 너무 민감
- 사전 가지치기(pre-pruning) : 트리 생성을 일찍 중단하는 전략
  - 트리의 최대 깊이, 리프의 최대 개수, 노드가 분할하기 위한 포인트의 최소 개수 등을 지정
- 가지치기(pruning) : 트리를 만든 후 데이터 포인트가 적은 노드를 삭제하거나 병합하는 전략
- sklearn에서는 사전 가지치기만 지원

```python
from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)
tree = DecisionTreeClassifier(random_state=0)
# tree = DecisionTreeClassifier(max_depth=4, random_state=0)
tree.fit(X_train, y_train)

print("train set score: {:.3f}".format(tree.score(X_train, y_train)))
print("test set score: {:.3f}".format(tree.score(X_test, y_test)))

# train set score: 1.000
# test set score: 0.937

# train set score: 0.988
# test set score: 0.951
```

- 모든 리프 노드가 순수 노드이므로 train set score는 100%가 나왔지만 test set score는 선형 모델에서의 정확도 보다 낮은 결과를 보임 
- **결정 트리의 깊이를 제한하지 않으면 트리는 무한정 깊어직 복잡해져 과대적합되고 새로운 데이터에 잘 일반화되지 않음**
- 사전 가지치기를 트리에 적용해서 훈련하면 과대적합이 줄어 train set score는 감소하지만 test set score는 개선시킴

```python
from sklearn.tree import export_graphviz
export_graphviz(tree, out_file="tree.dot", class_names=["malignity", "Benign"], feature_names=cancer.feature_names, impurity=False, filled=True)

import graphviz

with open("tree.dot") as f:
    dot_graph = f.read()
    display(graphviz.Source(dot_graph))
```

<img src="https://user-images.githubusercontent.com/58063806/109652775-8b300d80-7ba3-11eb-9462-eba8773f8275.png" width=50% />

위의 그림은 트리를 시각화한 것의 일부분으로 samples는 각 노드당 샘플 수, value는 클래스당 샘플의 수를 의미

```python
print("feature importance:\n", tree.feature_importances_)
```

<img src="https://user-images.githubusercontent.com/58063806/109653088-ecf07780-7ba3-11eb-8ba2-eea7c800e195.png" width=50% />

특성 중요도는 0 ~ 1사이의 값을 가지며 0은 전혀 사용되지 않았음을, 1은 완벽하게 타깃 클래스를 예측했다는 것을 의미 (특성 중요도의 전체 합은 1)

```python
import matplotlib.pyplot as plt
import numpy as np 

def plot_feature_importances_cancer(model):
    n_features = cancer.data.shape[1]
    plt.barh(np.arange(n_features), model.feature_importances_, align="center")
    plt.yticks(np.arange(n_features), cancer.feature_names)
    plt.xlabel('feature_importance')
    plt.ylabel('feature')
    plt.ylim(-1, n_features)
    
plot_feature_importances_cancer(tree)
```

<img src="https://user-images.githubusercontent.com/58063806/109653768-b8c98680-7ba4-11eb-9089-2407033384dc.png" width=60%/>

- 첫 번째 노드에서 사용한 worst radius 특성이 가장 중요한 특성으로 나타남 (첫 번째 노드에서 두 클래스를 꽤 잘 나누고 있음)
- 선형 모델의 계수와는 달리, 특성 중요도는 항상 양수이며 특성이 어떤 클래스를 지지하는지는 알 수 없음

```python
tree = mglearn.plots.plot_tree_not_monotone()
display(tree)
```

**두 개의 특성과 두 개의 클래스를 가진 데이터셋**

<img src="https://user-images.githubusercontent.com/58063806/109654461-8ec49400-7ba5-11eb-8c8e-a827af0dc225.png" width=40% />

- X[1] 특성만  사용, X[0]은 사용되지 않음
- 하지만 X[1]과 출력 클래스와 관계는 단순히 비례하거나 반비례하지 않음

#### 회귀 결정 트리

- 분류를 위한 결정 트리와 사용법이 매우 비슷하지만 **트리 기반 회귀 모델은 외삽 (extrapolation, 훈련 데이터 범위 밖의 포인트에 대해 예측)을 할 수 없음** 

```python
import os
import pandas as pd
ram_prices = pd.read_csv(os.path.join(mglearn.datasets.DATA_PATH, "ram_price.csv"))

plt.yticks(fontname="Arial")
plt.semilogy(ram_prices.date, ram_prices.price)
plt.xlabel("year")
plt.ylabel("price ($/Mbtye)")
```

<img src="https://user-images.githubusercontent.com/58063806/109655134-45287900-7ba6-11eb-8272-da87d3a34cf8.png" width=50% />

```python
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor

data_train = ram_prices[ram_prices.date < 2000]
data_test = ram_prices[ram_prices.date >= 2000]

# 가격 예측을 위해 날짜 특성만 사용
X_train = data_train.date[:, np.newaxis]
# 데이터와 타깃 사이의 관계를 간단하게 만들기 위해 로그 스케일로 변환
y_train = np.log(data_train.price) 

tree = DecisionTreeRegressor().fit(X_train, y_train)
linear_reg = LinearRegression().fit(X_train, y_train)

X_all = ram_prices.date[:, np.newaxis]

pred_tree = tree.predict(X_all)
pred_lr = linear_reg.predict(X_all)

# 로그 스케일을 다시 되돌림
price_tree = np.exp(pred_tree)
price_lr = np.exp(pred_lr)

plt.semilogy(data_train.date, data_train.price, label="train_data")
plt.semilogy(data_test.date, data_test.price, label="test_data")
plt.semilogy(ram_prices.date, price_tree, "--", label="tree predict")
plt.semilogy(ram_prices.date, price_lr, "--", label="linear regression predict")
plt.xlabel("year")
plt.ylabel("price ($/Mbtye)")
plt.legend()
```

<img src="https://user-images.githubusercontent.com/58063806/109656772-20350580-7ba8-11eb-9a12-67596bdd609f.png" width=50% />

- 선형 모델은 직선으로 데이터를 근사, 2000년 이후의 data도 꽤 정확히 예측
- 트리 모델은 훈련 데이터를 완벽하게 예측하지만 모델이 가진 데이터 범위 밖으로 나가면 단순히 마지막 데이터 포인트를 이용해 예측하는 단점

#### 장단점과 매개변수

- 결정 트리에서 모델 복잡도를 조절하는 매개변수는 사전 가지치기 매개변수로 max_depth, max_leaf_nodes, min_samples_leaf 중 하나만 지정해도 과대적합을 막기에 충분
- 만들어진 모델을 쉽게 시각화할 수 있어 이해가 쉬움
- 데이터 스케일의 영향을 받지 않으므로 특성의 정규화나 표준화 같은 전처리 과정이 필요없음
- 하지만 사전 가지치기를 이용해도 과대적합되는 경향이 있어 일반화 성능이 좋지 않음