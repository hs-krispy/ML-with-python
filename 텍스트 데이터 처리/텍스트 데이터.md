## 텍스트 데이터

- 주로 글자가 연결된 문자열로 표현 (단어로 구성된 문장에 정보를 담고있음)
- 텍스트 분석에서 데이터셋을 말뭉치 (corpus), 하나의 텍스트를 의미하는 각 데이터 포인트를 문서 (document)라고 함

IMBb - 영화 리뷰 데이터셋

-  1 ~ 10 까지의 점수가 존재 (7점 이상은 "양성", 4점 이하는 "음성"인 이진 분류)

```python
from sklearn.datasets import load_files

reviews_train = load_files("aclImdb/train/")
text_train, y_train = reviews_train.data, reviews_train.target
print("text_train의 타입:", type(text_train))
print("text_train의 길이:", len(text_train))
print("text_train[6]:", text_train[6])

text_train의 타입: <class 'list'>
text_train의 길이: 25000
text_train[6]: b"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending."
```

text_train 리스트의 길이는 25000이고 각 항목은 리뷰 한 개에 대한 문자열에 해당함

```python
text_train = [doc.replace(b"<br />", b" ") for doc in text_train]
```

text_train의 항목의 타입은 bytes 타입에 해당

br 태그를 공백으로 변환해서 데이터를 정리

```python
print("클래스별 샘플 수 (훈련 데이터):", np.bincount(y_train))
# 클래스별 샘플 수 (훈련 데이터): [12500 12500]
```

양성 클래스와 음성 클래스를 같은 비율로 수집된 것을 확인

```python
reviews_test = load_files("aclImdb/test/")
text_test, y_test = reviews_test.data, reviews_test.target
print("테스트 데이터의 문서 수:", len(text_test))
print("클래스별 샘플 수 (테스트 데이터):", np.bincount(y_test))
text_test = [doc.replace(b"<br />", b" ") for doc in text_test]

# 테스트 데이터의 문서 수: 25000
# 클래스별 샘플 수 (테스트 데이터): [12500 12500]
```

텍스트 데이터는 머신러닝 모델이 다룰 수 있는 형태가 아니므로 **문자열 표현을 수치 표현으로 바꿔야 함**

#### BOW

- Bag of Words (텍스트를 담는 가방)
- 머신러닝에서 텍스트를 표현하는 방법 중 하나로 가장 간단하지만 효과적이면서 널리 쓰임
- 장, 문단, 문장, 서식 같은 입력 텍스트의 구조 대부분을 잃고, **각 단어가 말뭉치에 있는 텍스트에 얼마나 많이 나타나는지만 헤아림**

1. **토큰화** - 각 문서를 **문서에 포함된 단어(토큰)로 나눔** (공백이나 구두점 등을 기준으로 분리)
2. **어휘 사전 구축** - 모든 문서에 나타난 **모든 단어의 어휘를 모으고 번호를 매김** (알파벳 순서)
3. **인코딩** - 어휘 사전의 **단어가 문서마다 몇 번이나 나타나는지를 헤아림**

```python
bards_words = ["The fool doth think he is wise,", "but the wise man knows himself to be a fool"]

from sklearn.feature_extraction.text import CountVectorizer

vect = CountVectorizer()
vect.fit(bards_words)

print("어휘 사전의 크기:", len(vect.vocabulary_))
print("어휘 사전의 내용:\n", vect.vocabulary_)

# 어휘 사전의 크기: 13
# 어휘 사전의 내용:
# {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}
```

CountVectorizer - 데이터를 토큰으로 나누고 어휘 사전을 구축해서 vocabulary_ 속성에 저장

```python
bag_of_words = vect.transform(bards_words)
print("BOW:", repr(bag_of_words))
# BOW: <2x13 sparse matrix of type '<class 'numpy.int64'>' with 16 stored elements in Compressed Sparse Row format>
print("BOW:\n", bag_of_words)
```

<img src="https://user-images.githubusercontent.com/58063806/115884115-8132d880-a489-11eb-9898-874a57b85363.png" width=15% />

- BOW 표현은 0이 아닌 값만 저정하는 SciPy 희소 행렬로 저장되어 있음
- 대부분의 문서는 어휘 사전에 있는 단어 중 일부만 포함하므로, 즉 특성 배열의 대부분의 원소가 0이므로 희소 행렬을 사용
  - 값이 0인 원소를 모두 저장하는 것은 엄청난 메모리 낭비임
- 2 x 13의 크기로 각각의 행은 하나의 데이터 포인트를 나타내고, 각 특성은 어휘 사전에 있는 각 단어에 대응함

0인 원소도 모두 포함된 결과

```python
print("BOW의 밀집 표현:\n", bag_of_words.toarray())
# BOW의 밀집 표현:
# [[0 0 1 1 1 0 1 0 0 1 1 0 1]
# [1 1 0 1 0 1 0 1 1 1 0 1 1]]
```

```python
vect = CountVectorizer().fit(text_train)
X_train = vect.transform(text_train)
print("X_train:\n", repr(X_train))
# X_train: <25000x74849 sparse matrix of type '<class 'numpy.int64'>' with 3431196 stored elements in Compressed Sparse Row format>
```

이 어휘 사전은 단어를 74849개 담고 있음

```python
feature_names = vect.get_feature_names()
print("특성 개수:", len(feature_names))
# 특성 개수: 74849

print("처음 20개 특성:\n", feature_names[:20])
# 처음 20개 특성:
# ['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']

print("20010에서 20030까지 특성:\n", feature_names[20010:20030])
# 20010에서 20030까지 특성:
# ['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl', 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']

print("매 2000번째 특성:\n", feature_names[::2000])
# 매 2000번째 특성:
# ['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful', 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher', 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse', 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']
```

CountVectorizer 객체의 get_feature_names 메서드는 각 특성에 해당하는 단어를 리스트로 반환

```python
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)
print("교차 검증 평균 점수: {:.2f}".format(np.mean(scores)))
# 교차 검증 평균 점수: 0.88

from sklearn.model_selection import GridSearchCV

param_grid = {"C": [0.001, 0.01, 0.1, 1, 10]}
grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid.fit(X_train, y_train)
print("최적의 교차 검증 점수: {:.2f}".format(grid.best_score_))
print("최적의 매개변수:", grid.best_params_)
# 최적의 교차 검증 점수: 0.89
# 최적의 매개변수: {'C': 0.1}
```

C = 0.1에서 교차 검증 점수 89%를 얻음

```python
X_test = vect.transform(text_test)
print("테스트 점수: {:.2f}".format(grid.score(X_test, y_test)))
# 테스트 점수: 0.88
```

테스트 세트의 일반화 점수로는 88%를 얻음



CountVectorizer는 정규표현식을 사용해 토큰을 추출

- 기본적으로 "\b\w\w + \b" 정규표현식을 사용 (경계가 구분되고 적어도 둘 이상의 문자나 숫자가 연속된 단어를 찾음)
- 모든 단어를 소문자로 바꿈
- 이러한 매커니즘은 잘 작동하지만 앞선 경우의 숫자 등과 같이 **의미 없는 특성을 많이 생성**
- 이를 줄이기 위해 **적어도 두 개의 문서(또는 다섯 개의 문서)에 나타난 토큰만을 사용**
  - min_df 매개변수로 토큰이 나타날 최소 문서 개수를 지정 가능

```python
vect = CountVectorizer(min_df=5).fit(text_train)
X_train = vect.transform(text_train)
print("min_df로 제한한 X_train:", repr(X_train))
# min_df로 제한한 X_train: <25000x27271 sparse matrix of type '<class 'numpy.int64'>' with 3354014 stored elements in Compressed Sparse Row format>
```

토큰이 적어도 다섯 번의 문서에 나타나야 하므로, 기존의 74849개의 특성에 비해 1/3 정도인 27271개로 줄어듬

```python
feature_names = vect.get_feature_names()

print("처음 50개 특성:\n", feature_names[:50])
# 처음 50개 특성:
#  ['00', '000', '007', '00s', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '100th', '101', '102', '103', '104', '105', '107', '108', '10s', '10th', '11', '110', '112', '116', '117', '11th', '12', '120', '12th', '13', '135', '13th', '14', '140', '14th', '15', '150', '15th', '16', '160', '1600', '16mm', '16s', '16th']

print("20010부터 20030까지 특성:\n", feature_names[20010:20030])
# 20010부터 20030까지 특성:
# ['repentance', 'repercussions', 'repertoire', 'repetition', 'repetitions', 'repetitious', 'repetitive', 'rephrase', 'replace', 'replaced', 'replacement', 'replaces', 'replacing', 'replay', 'replayable', 'replayed', 'replaying', 'replays', 'replete', 'replica']

print("매 700번째 특성:\n", feature_names[::700])
# 매 700번째 특성:
# ['00', 'affections', 'appropriately', 'barbra', 'blurbs', 'butchered', 'cheese', 'commitment', 'courts', 'deconstructed', 'disgraceful', 'dvds', 'eschews', 'fell', 'freezer', 'goriest', 'hauser', 'hungary', 'insinuate', 'juggle', 'leering', 'maelstrom', 'messiah', 'music', 'occasional', 'parking', 'pleasantville', 'pronunciation', 'recipient', 'reviews', 'sas', 'shea', 'sneers', 'steiger', 'swastika', 'thrusting', 'tvs', 'vampyre', 'westerns']
```

숫자의 길이와 희귀한 단어 또는 철자가 틀린 단어들이 사라진 것을 볼 수 있음

```python
grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid.fit(X_train, y_train)
print("최적의 교차 검증 점수: {:.2f}".format(grid.best_score_))
# 최적의 교차 검증 점수: 0.89
```

교차 검증 점수에 변화가 있지는 않았지만 특성의 개수가 적어짐에 따라 처리 속도가 약간 빨라짐

#### 불용어

- 의미 없는 단어를 제거하는 또 다른 방법으로 너무 빈번해서 유용하지 않은 단어를 제외
  - 언어별 불용어 (stopword) 목록 사용
  - 너무 자주 나타나는 단어를 제외

```python
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
print("불용어 개수:", len(ENGLISH_STOP_WORDS))
print("매 10번째 불용어:\n", list(ENGLISH_STOP_WORDS)[::10])

# 불용어 개수: 318
# 매 10번째 불용어:
# ['might', 'yourselves', 'is', 'everywhere', 'against', 'but', 'get', 'next', 'and', 'whenever', 'empty', 'yourself', 'whereas', 'can', 'hereby', 'neither', 'i', 'they', 'while', 'whether', 'down', 'between', 'than', 'besides', 'behind', 'since', 'nevertheless', 'my', 'alone', 'nine', 'before', 'will']
```

```python
from sklearn.feature_extraction.text import CountVectorizer

# english - 내장된 불용어
vect = CountVectorizer(min_df=5, stop_words="english").fit(text_train)
X_train = vect.transform(text_train)
print("불용어가 제거된 X_train:\n", repr(X_train))
# 불용어가 제거된 X_train: <25000x26967 sparse matrix of type '<class 'numpy.int64'>' with 2164624 stored elements in Compressed Sparse Row format>
```

27271개에서 26967개로 특성 305개가 줄어듬

```python
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

param_grid = {"C": [0.001, 0.01, 0.1, 1, 10]}
grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid.fit(X_train, y_train)
print("최상의 교차 검증 점수: {:.2f}".format(grid.best_score_))
# 최상의 교차 검증 점수: 0.88
```

약간 감소한 88%의 성능을 나타내는데 모델 해석에 도움이 되는 것 같지는 않으므로, 해당 불용어 목록은 도움이 안 됨 **(고정된 불용어 목록은 모델이 데이터셋만 보고 불용어를 골라내기 어려운 작은 데이터셋에서나 도움이 됨)**

#### tf-idf

- 중요하지 않아 보이는 특성을 제외하는 대신, 얼마나 의미 있는 특성인지를 계산해서 스케일을 조정하는 방식
- 말뭉치의 다른 문서보다 특정 문서에 자주 나타나는 단어에 높은 가중치를 주는 방법

**문서 d에 있는 단어 w에 대한 tf-idf 점수** 

tfidf(w, d) = tf(log(N + 1 / Nw + 1) + 1)

> N은 훈련 세트에 있는 문서의 개수, Nw는 단어 w가 나타난 훈련 세트 문서의 개수,
>
> tf(단어빈도수)는 단어 w가 대상 문서d에 나타난 횟수
>
> 위의 식을 계산 후 L2 normalization을 적용 (문서의 길이에 영향을 없애기 위함)

tf-idf는 훈련 데이터의 통계적 속성을 사용하기 때문에 파이프라인을 이용해서 그리드 서치를 제대로 적용

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline

pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())
param_grid = {"logisticregression__C": [0.001, 0.01, 0.1, 1, 10]}

grid = GridSearchCV(pipe, param_grid, cv=5)
grid.fit(text_train, y_train)
print("최상의 교차 검증 점수: {:.2f}".format(grid.best_score_))
# 최상의 교차 검증 점수: 0.89
```

여기서는 tf-idf가 성능에 큰 영향을 주지 못함

tf-idf 변환은 문서를 구별하는 단어를 찾는 방법이지만 **완전히 비지도 학습 (긍적적 리뷰와 부정적 리뷰 레이블과 꼭 관계가 있지 않음)**

```python
import numpy as np

vectorizer = grid.best_estimator_.named_steps["tfidfvectorizer"]
X_train = vectorizer.transform(text_train)
max_value = X_train.max(axis=0).toarray().ravel()
sorted_by_tfidf = max_value.argsort()
feature_names = np.array(vectorizer.get_feature_names())

print("가장 낮은 tfidf를 가진 특성:\n", feature_names[sorted_by_tfidf[:20]])
# 가장 낮은 tfidf를 가진 특성:
 ['suplexes' 'gauche' 'hypocrites' 'oncoming' 'galadriel' 'songwriting'
 'cataclysmic' 'sylvain' 'emerald' 'mclaughlin' 'oversee' 'pressuring'
 'uphold' 'thieving' 'inconsiderate' 'ware' 'denim' 'booed' 'reverting'
 'spacious']
print("가장 높은 tfidf를 가진 특성:\n", feature_names[sorted_by_tfidf[-20:]])
# 가장 높은 tfidf를 가진 특성:
 ['muppet' 'brendan' 'zatoichi' 'dev' 'demons' 'lennon' 'bye' 'weller'
 'woo' 'sasquatch' 'botched' 'xica' 'darkman' 'casper' 'doodlebops'
 'steve' 'smallville' 'wei' 'scanners' 'pokemon']
```

- tf-idf가 낮은 특성은 전체 문서에 걸쳐 매우 많이 나타나거나, 조금씩만 사용되거나, 매우 긴 문서에서만 사용

> 전체 문서에 걸쳐 많이 나타나면 idf값이 1에 가깝게 되고, 조금씩 사용되거나 매우 긴 문서에서만 사용되면 L2 정규화에 의해 tf-idf 값이 작아짐

- tf-idf가 높은 특성은 어떤 쇼나 영화를 나타내는 경우가 많음
  - "pokemon", "smallville", "doodlebops", "scanners"는 매우 자주 나타나는 경향

```python
sorted_by_tfidf = np.argsort(vectorizer.idf_)
print("가장 낮은 idf를 가진 특성:\n", feature_names[sorted_by_tfidf[:100]])
# 가장 낮은 idf를 가진 특성:
 ['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'
 'was' 'as' 'on' 'movie' 'not' 'br' 'have' 'one' 'be' 'film' 'are' 'you'
 'all' 'at' 'an' 'by' 'so' 'from' 'like' 'who' 'they' 'there' 'if' 'his'
 'out' 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'good' 'can' 'more'
 'when' 'time' 'up' 'very' 'even' 'only' 'no' 'would' 'my' 'see' 'really'
 'story' 'which' 'well' 'had' 'me' 'than' 'much' 'their' 'get' 'were'
 'other' 'been' 'do' 'most' 'don' 'her' 'also' 'into' 'first' 'made' 'how'
 'great' 'because' 'will' 'people' 'make' 'way' 'could' 'we' 'bad' 'after'
 'any' 'too' 'then' 'them' 'she' 'watch' 'think' 'acting' 'movies' 'seen'
 'its']
```

- idf값이 낮은 단어, 즉 자주 나타나서 덜 중요하다고 생각되는 단어들은 대부분 "the", "no" 같은 영어의 불용어
- 일부는 "movie", "film", "time", "story" 같이 영화 리뷰에서만 나타나는 단어
- 흥미로운 것은 "good", "great", "bad"도 매우 자주 나타나는 단어로 감성 분석에는 매우 중요하지만 tf-idf의 관점에서는 덜 중요한 단어

#### 모델 계수 조사

가장 큰 값의 계수와 해당 단어를 확인

```python
import mglearn

mglearn.tools.visualize_coefficients(grid.best_estimator_.named_steps["logisticregression"].coef_[0], feature_names, n_top_features=40)
```

<img src="https://user-images.githubusercontent.com/58063806/115949305-1e842000-a50f-11eb-8fc2-804da9b47fbd.png" width=100% />

- 가장 큰 계수 40개와 가장 작은 계수 40개
- 음수 계수는 모델에서 부정적인 리뷰를 의미하는 단어
- 양수 계수는 긍정적인 리뷰의 단어