## 텍스트 데이터

- BOW 표현 방식은 단어의 순서가 완전히 무시된다는 큰 단점이 존재
  - "it's bad, not good at all"과 "it's good, not bad at all"이 완전히 동일하게 변환
- 토큰 하나의 횟수만 고려하지 않고 옆에 있는 두세개의 토큰을 함께 고려하는 방식 사용
  - 토큰 두개 - 바이그램(bigram)
  - 세개 - 트라이그램(trigram)
  - 일반적으로 연속된 토큰 - n-그램(n-gram)

```python
from sklearn.feature_extraction.text import CountVectorizer

bards_words = ["The fool doth think he is wise,", "but the wise man knows himself to be a fool"]

cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)
print("어휘 사전 크기:", len(cv.vocabulary_))
print("어휘 사전:\n", cv.get_feature_names())
# 어휘 사전 크기: 13
# 어휘 사전:
 ['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']
```

ngram_range : 연속된 토큰의 최소 길이와 최대 길이를 지정 (위에서는 최소, 최대 길이 둘 다 1, 토큰 하나(유니그램)를 의미)

```python
cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)
print("어휘 사전 크기:", len(cv.vocabulary_))
print("어휘 사전:\n", cv.get_feature_names())
# 어휘 사전 크기: 14
# 어휘 사전:
 ['be fool', 'but the', 'doth think', 'fool doth', 'he is', 'himself to', 'is wise', 'knows himself', 'man knows', 'the fool', 'the wise', 'think he', 'to be', 'wise man']
```

- 연속된 토큰의 수가 커지면 보통 특성이 더 구체적이고 많이 생성됨

```python
print("변환된 데이터 (밀집 배열)\n", cv.transform(bards_words).toarray())
# 변환된 데이터 (밀집 배열)
 [[0 0 1 1 1 0 1 0 0 1 0 1 0 0]
 [1 1 0 0 0 1 0 1 1 0 1 0 1 1]]
```

- 단어 하나가 큰 의미를 가진 경우가 많아서 대부분의 애플리케이션에서 토큰의 최소 길이는 1로 지정
  - 많은 경우에 바이그램을 추가하면 도움이 되고 많게는 5-그램까지는 도움이 되지만 특성의 개수가 매우 많아지고 구체적인 특성이 많아져서 과대적합의 가능성이 높아짐
  - **이론상 바이그램의 수는 유니그램 수의 제곱, 트라이그램의 수는 유니그램의 세제곱**이 되므로 특성의 개수가 많이 늘어남 (실제로는 언어의 구조상 이보다는 적은 개수)

**바이그램, 트라이그램 적용**

```python
cv = CountVectorizer(ngram_range=(1, 2)).fit(bards_words)
print("어휘 사전 크기:", len(cv.vocabulary_))
print("어휘 사전:\n", cv.get_feature_names())
# 어휘 사전 크기: 27
# 어휘 사전:
 ['be', 'be fool', 'but', 'but the', 'doth', 'doth think', 'fool', 'fool doth', 'he', 'he is', 'himself', 'himself to', 'is', 'is wise', 'knows', 'knows himself', 'man', 'man knows', 'the', 'the fool', 'the wise', 'think', 'think he', 'to', 'to be', 'wise', 'wise man']
    
cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)
print("어휘 사전 크기:", len(cv.vocabulary_))
print("어휘 사전:\n", cv.get_feature_names())
# 어휘 사전 크기: 39
# 어휘 사전:
 ['be', 'be fool', 'but', 'but the', 'but the wise', 'doth', 'doth think', 'doth think he', 'fool', 'fool doth', 'fool doth think', 'he', 'he is', 'he is wise', 'himself', 'himself to', 'himself to be', 'is', 'is wise', 'knows', 'knows himself', 'knows himself to', 'man', 'man knows', 'man knows himself', 'the', 'the fool', 'the fool doth', 'the wise', 'the wise man', 'think', 'think he', 'think he is', 'to', 'to be', 'to be fool', 'wise', 'wise man', 'wise man knows']
```

```python
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())
param_grid = {"logisticregression__C": [0.001, 0.01, 0.1, 1, 10, 100],
"tfidfvectorizer__ngram_range": [(1, 1), (1, 2), (1, 3)]}

grid = GridSearchCV(pipe, param_grid, cv=5)
grid.fit(text_train, y_train)
print("최상의 교차 검증 점수: {:.2f}".format(grid.best_score_))
print("최적의 매개변수:\n", grid.best_params_)
# 최상의 교차 검증 점수: 0.91
# 최적의 매개변수:
 {'logisticregression__C': 100, 'tfidfvectorizer__ngram_range': (1, 3)}
```

```python
import mglearn
import matplotlib.pyplot as plt

scores = grid.cv_results_["mean_test_score"].reshape(-1, 3).T
heatmap = mglearn.tools.heatmap(
    scores, xlabel="C", ylabel="ngram_range", cmap="viridis", fmt="%.3f",
    xticklabels=param_grid["logisticregression__C"],
    yticklabels=param_grid["tfidfvectorizer__ngram_range"]
)
plt.colorbar(heatmap)
```

<img src="https://user-images.githubusercontent.com/58063806/115989189-2b307300-a5f8-11eb-87a5-d8dca5dbe053.png" width=50%/>

바이그램과 트라이그램을 적용했을때 성능이 1% 이상 향상되는 것을 볼 수 있음

```python
import numpy as np

vect = grid.best_estimator_.named_steps["tfidfvectorizer"]
feature_names = np.array(vect.get_feature_names())
coef = grid.best_estimator_.named_steps["logisticregression"].coef_
mglearn.tools.visualize_coefficients(coef[0], feature_names, n_top_features=40)
```

<img src="https://user-images.githubusercontent.com/58063806/115989269-8d897380-a5f8-11eb-959c-1b3ec2da754e.png" width=100% />

- 유니그램 모델에서는 없던 "worth"가 추가됨

- not worth는 부정적인 리뷰를 의미하지만 well worth는 긍정적인 리뷰를 암시

**트라이그램 특성들만 추출**

```python
mask = np.array([len(feature.split(" ")) for feature in feature_names]) == 3
mglearn.tools.visualize_coefficients(coef.ravel()[mask], feature_names[mask], n_top_features=40)
```

<img src="https://user-images.githubusercontent.com/58063806/115989492-99296a00-a5f9-11eb-94c1-7591a4cbe597.png" width=100% />

- 영향력이 큰 바이그램과 트라이그램의 대부분은 독립적일 땐 큰 의미가 없는 단어들로 구성

