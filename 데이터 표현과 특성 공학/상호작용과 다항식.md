## 상호작용과 다항식

- 원본 데이터에 상호작용과 다항식을 추가함으로써 특성을 풍부하게 만들 수 있음

```python
X, y = mglearn.datasets.make_wave(n_samples=120)
line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)

kb = KBinsDiscretizer(n_bins=10, strategy="uniform", encode="onehot-dense")
X_binned = kb.fit_transform(X)
line_binned = kb.transform(line)

X_combined = np.hstack([X, X_binned])
# (120, 11)
reg = LinearRegression().fit(X_combined, y)

line_combined = np.hstack([line, line_binned])

plt.plot(line, reg.predict(line_combined), label="원본 특성을 더한 선형 회귀")

plt.vlines(kb.bin_edges_[0], -3, 3, linewidth=1, alpha=.2)
plt.plot(X, y, "o", c="k")
plt.ylabel("회귀 출력")
plt.xlabel("입력 특성")
plt.legend(loc="best")
```

<img src="https://user-images.githubusercontent.com/58063806/113000831-29b38c80-91ab-11eb-8c92-e50f2591a7a1.png" width=50% />

- 각 구간의 상숫값(절편)외에 원래 특성(기울기)를 추가해서 학습
- 학습된 기울기는 양수이고 모든 구간에 걸쳐 동일

```python
X_product = np.hstack([X_binned, X * X_binned])
# (120, 20)
reg = LinearRegression().fit(X_product, y)

line_product = np.hstack([line_binned, line * line_binned])

plt.plot(line, reg.predict(line_product), label="원본 특성을 곱한 선형 회귀")

plt.vlines(kb.bin_edges_[0], -3, 3, linewidth=1, alpha=.2)
plt.plot(X, y, "o", c="k")
plt.ylabel("회귀 출력")
plt.xlabel("입력 특성")
plt.legend(loc="best")
```

<img src="https://user-images.githubusercontent.com/58063806/113002474-c4609b00-91ac-11eb-948a-991427627c8a.png" width=50% />

- 데이터 포인트가 있는 구간과 x 축 사이의 상호작용 특성(구간 특성과 원본 특성의 곱)을 추가
  - 이 값은 구간 안에서는 원본 특성이고 다른 곳에서는 0

```python
from sklearn.preprocessing import PolynomialFeatures

# X ** 10 까지
# include_bias=True는 절편에 해당하는 1 값 추가
poly = PolynomialFeatures(degree=10, include_bias=False)
X_poly = poly.fit_transform(X)

print(X[1])
# [2.70428584]
print(X_poly[1])
# [2.70428584e+00 7.31316190e+00 1.97768801e+01 5.34823369e+01 1.44631526e+02 3.91124988e+02 1.05771377e+03 2.86036036e+03 7.73523202e+03 2.09182784e+04]

print("항 이름:", poly.get_feature_names())
# 항 이름: ['x0', 'x0^2', 'x0^3', 'x0^4', 'x0^5', 'x0^6', 'x0^7', 'x0^8', 'x0^9', 'x0^10']

reg = LinearRegression().fit(X_poly, y)

line_poly = poly.transform(line)

plt.plot(line, reg.predict(line_poly), label="다항 선형 회귀")
plt.plot(X, y, "o", c="k")
plt.ylabel("회귀 출력")
plt.xlabel("입력 특성")
plt.legend(loc="best")
```

<img src="https://user-images.githubusercontent.com/58063806/113003994-35ed1900-91ae-11eb-9b7c-b7f874dae88d.png" width=50% />

- 다항식 특성은 1차원 데이터셋에서도 매우 부드러운 곡선을 만듬
- 고차원 다항식은 데이터가 부족한 영역에서 민감하게 동작

```python
from sklearn.svm import SVR

for gamma in [1, 10]:
    svr = SVR(gamma=gamma).fit(X, y)
    plt.plot(line, svr.predict(line), label="SVR gamma={}".format(gamma))
    
plt.plot(X, y, "o", c="k")
plt.ylabel("회귀 출력")
plt.xlabel("입력 특성")
plt.legend(loc="best")
```

<img src="https://user-images.githubusercontent.com/58063806/113004755-ce839900-91ae-11eb-8010-856fc47718c0.png" width=50% />

- 더 복잡한 모델인 커널 SVM을 사용해 특성 데이터를 변환하지 않고 다항 회귀와 비슷한 예측을 생성

```python
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

boston = load_boston()
X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=0)

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train_scaled)
X_test_poly = poly.transform(X_test_scaled)

print(X_train.shape)
# (379, 13)
print(X_train_poly.shape)
# (379, 105)

print("다항 특성 이름:\n", poly.get_feature_names())
#  ['1', 'x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x0^2', 'x0 x1', 'x0 x2', 'x0 x3', 'x0 x4', 'x0 x5', 'x0 x6', 'x0 x7', 'x0 x8', 'x0 x9', 'x0 x10', 'x0 x11', 'x0 x12', 'x1^2', 'x1 x2', 'x1 x3', 'x1 x4', 'x1 x5', 'x1 x6', 'x1 x7', 'x1 x8', 'x1 x9', 'x1 x10', 'x1 x11', 'x1 x12', 'x2^2', 'x2 x3', 'x2 x4', 'x2 x5', 'x2 x6', 'x2 x7', 'x2 x8', 'x2 x9', 'x2 x10', 'x2 x11', 'x2 x12', 'x3^2', 'x3 x4', 'x3 x5', 'x3 x6', 'x3 x7', 'x3 x8', 'x3 x9', 'x3 x10', 'x3 x11', 'x3 x12', 'x4^2', 'x4 x5', 'x4 x6', 'x4 x7', 'x4 x8', 'x4 x9', 'x4 x10', 'x4 x11', 'x4 x12', 'x5^2', 'x5 x6', 'x5 x7', 'x5 x8', 'x5 x9', 'x5 x10', 'x5 x11', 'x5 x12', 'x6^2', 'x6 x7', 'x6 x8', 'x6 x9', 'x6 x10', 'x6 x11', 'x6 x12', 'x7^2', 'x7 x8', 'x7 x9', 'x7 x10', 'x7 x11', 'x7 x12', 'x8^2', 'x8 x9', 'x8 x10', 'x8 x11', 'x8 x12', 'x9^2', 'x9 x10', 'x9 x11', 'x9 x12', 'x10^2', 'x10 x11', 'x10 x12', 'x11^2', 'x11 x12', 'x12^2']
```

- 원래 특성의 제곱과 가능한 두 특성의 조합들을 포함한 특성으로 확장

```python
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

ridge = Ridge().fit(X_train_scaled, y_train)
print("상호작용 특성이 없을 때 점수: {:.3f}".format(ridge.score(X_test_scaled, y_test)))
# 상호작용 특성이 없을 때 점수: 0.621
ridge = Ridge().fit(X_train_poly, y_train)
print("상호작용 특성이 있을 때 점수: {:.3f}".format(ridge.score(X_test_poly, y_test)))
# 상호작용 특성이 있을 때 점수: 0.753


tree = DecisionTreeRegressor(random_state=0).fit(X_train_scaled, y_train)
print("상호작용 특성이 없을 때 점수: {:.3f}".format(tree.score(X_test_scaled, y_test)))
# 상호작용 특성이 없을 때 점수: 0.613
tree = DecisionTreeRegressor(random_state=0).fit(X_train_poly, y_train)
print("상호작용 특성이 있을 때 점수: {:.3f}".format(tree.score(X_test_poly, y_test)))
# 상호작용 특성이 있을 때 점수: 0.626

rf = RandomForestRegressor(n_estimators=100, random_state=0).fit(X_train_scaled, y_train)
print("상호작용 특성이 없을 때 점수: {:.3f}".format(rf.score(X_test_scaled, y_test)))
# 상호작용 특성이 없을 때 점수: 0.795
rf = RandomForestRegressor(n_estimators=100, random_state=0).fit(X_train_poly, y_train)
print("상호작용 특성이 있을 때 점수: {:.3f}".format(rf.score(X_test_poly, y_test)))
# 상호작용 특성이 있을 때 점수: 0.774
```

- 상호작용과 다항식 특성을 추가했을 때 선형 모델인 Ridge의 성능은 크게 향상
- 결정 트리는 성능의 변화가 거의 없음
- 랜덤 포레스트는 특성을 추가하지 않아도 높은 성능을 보이지만 특성을 추가하면 오히려  성능이 약간 저하됨

> 랜덤 포레스트는 결정 트리의 앙상블 형태로 특성들의 상호작용 효과를 표현할 수 있는 모델이기 때문으로 추정 

