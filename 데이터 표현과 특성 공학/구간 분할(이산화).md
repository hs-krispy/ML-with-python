### 구간 분할 (이산화)

- 연속형 데이터에 강력한 선형 모델을 만드는 방법
- **한 특성을 여러 특성으로 나누는 것 (이산화)**

```python
import mglearn
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor

plt.rc('font', family="Malgun Gothic")
plt.rcParams['axes.unicode_minus'] = False

X, y = mglearn.datasets.make_wave(n_samples=120)
line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)

reg = DecisionTreeRegressor(min_samples_leaf=3).fit(X, y)
plt.plot(line, reg.predict(line), label="결정 트리")

reg = LinearRegression().fit(X, y)
plt.plot(line, reg.predict(line), "--", label="선형 회귀")
plt.plot(X, y, "o", c="k")
plt.ylabel("회귀 출력")
plt.xlabel("입력 특성")
plt.legend(loc="best")
```

<img src="https://user-images.githubusercontent.com/58063806/112845529-e9d3a300-90df-11eb-8b25-5b6a6feb89fe.png" width=50% />

- 구간의 경계를 정의
  - 균일한 너비로 (구간의 경계 간의 거리가 동일하게)
  - 데이터의 분위를 사용

```python
from sklearn.preprocessing import KBinsDiscretizer

kb = KBinsDiscretizer(n_bins=10, strategy="uniform")
kb.fit(X)
print("bin edges: \n", kb.bin_edges_)
# bin edges: 
# [array([-2.9668673 , -2.37804841, -1.78922951, -1.20041062, -0.61159173, -0.02277284,  0.56604605,  1.15486494,  1.74368384,  2.33250273, 2.92132162])]

X_binned = kb.transform(X)

print(X[:10])
# [[-0.75275929]
# [ 2.70428584]
# [ 1.39196365]
# [ 0.59195091]
# [-2.06388816]
# [-2.06403288]
# [-2.65149833]
# [ 2.19705687]
# [ 0.60669007]
# [ 1.24843547]]
print(X_binned.toarray()[:10])
# [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
# [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
# [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
# [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
# [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
# [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
# [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
# [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
# [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
# [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]
```

KBinsDiscretizer - 데이터의 구간을 나누고 각 구간에 one-hot-encoding을 적용 (각 데이터 포인트가 어느 구간에 속했는지 변환)

> 한 번에 여러 개의 특성에 적용이 가능

```python
# encode="onehot-dense" : 바로 원-핫-인코딩된 밀집 배열을 return
kb = KBinsDiscretizer(n_bins=10, strategy="uniform", encode="onehot-dense")
X_binned = kb.fit_transform(X)
line_binned = kb.transform(line)

reg = LinearRegression().fit(X_binned, y)
plt.plot(line, reg.predict(line_binned), label="구간 선형 회귀")

reg = DecisionTreeRegressor(min_samples_split=3).fit(X_binned, y)
plt.plot(line, reg.predict(line_binned), label="구간 결정 트리")

plt.plot(X, y, "o", c="k")
plt.vlines(kb.bin_edges_[0], -3, 3, linewidth=1, alpha=.2)
plt.ylabel("회귀 출력")
plt.xlabel("입력 특성")
plt.legend(loc="best")
```

<img src="https://user-images.githubusercontent.com/58063806/112858834-396c9b80-90ed-11eb-932d-a23823924056.png" width=50%/>

- 선형 회귀 모델과 결정 트리가 같은 예측을 만들어내서 파선과 실선이 완전히 겹침
- 각 구간 안에서는 특성의 값이 상수, 어떤 모델이든 그 구간의 포인트에 대해서는 같은 값을 예측
- 선형 모델이 훨씬 유연해진 반면 결정 트리는 덜 유연해짐

> 트리 모델은 데이터를 자유롭게 나눠 학습할 수 있으므로 특성 값을 구간으로 나누는 것이 도움이 되지 않음

- 용량이 매우 크고 고차원 데이터셋이라 선형 모델을 사용할 경우에 구간 분할이 좋은 방법이 될 수 있음