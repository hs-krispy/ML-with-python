## 평가 지표와 측정

- 평가 지표를 선택할 때 머신러닝 애플리케이션의 최종 목표 (고차원적인 목표)를 생각해야 함
- 머신러닝 애플리케이션에서 특정 알고리즘을 선택하여 나타난 결과를 **비즈니스 임팩트**라고 함



#### 이진 분류의 평가 지표

- 잘못 분류한 샘플의 수가 원하는 정보의 전부는 아니므로, 정확도만으로 예측 성능을 측정하기에 부족할 때가 있음

EX) 암을 조기 발견하는 애플리케이션, 음성이면 건강, 양성이면 추가 검사를 받아야 함

**거짓 양성(false positive)** - 건강한 사람을 양성으로 분류하면 추가 검사를 받게하며 환자에게 비용손실과 불편함을 초래

**거짓 음성(false negative)** - 암에 걸린 사람을 음성으로 분류하면 제대로 치료를 받지 못하고 건강에 치명적

해당 예시에서는 거짓 음성을 최대한 피해야하는 반면, 거짓 양성은 비교적 중요도가 낮음 **(일반적으로도 거짓 양성의 중요도와 거짓 음성의 중요도가 비슷한 경우는 매우 드뭄)**

**불균형 데이터셋**

```python
import numpy as np
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.dummy import DummyClassifier

digits = load_digits()
y = digits.target == 9

X_train, X_test, y_train, y_test = train_test_split(digits.data, y, random_state=0)

dummy_majority = DummyClassifier(strategy="most_frequent").fit(X_train, y_train)
pred_most_frequent = dummy_majority.predict(X_test)
print("예측된 레이블의 레이블:", np.unique(pred_most_frequent))
print("테스트 점수: {:.2f}".format(dummy_majority.score(X_test, y_test)))
# 예측된 레이블의 레이블: [False]
# 테스트 점수: 0.90
```

항상 다수인 클래스를 예측하도록 설정, 한 클래스만 예측해서 90%의 정확도를 얻음

```python
from sklearn.linear_model import LogisticRegression

dummy = DummyClassifier().fit(X_train, y_train)
pred_dummy = dummy.predict(X_test)
print("dummy 점수: {:.2f}".format(dummy.score(X_test, y_test)))
# dummy 점수: 0.84

logreg = LogisticRegression(C=0.1).fit(X_train, y_train)
pred_logreg = logreg.predict(X_test)
print("logreg 점수: {:.2f}".format(logreg.score(X_test, y_test)))
# logreg 점수: 0.98
```

무작위로 예측하도록 설정한 분류기는 정확도를 봤을때는 결과가 더 안좋음

반면 LogisticRegression은 매우 좋은 성능

이러한 결과가 실제로 유용한 것인지 판단하기 어려움 **(불균형 데이터셋에서 예측 성능을 정량화하는 데 정확도는 적절한 측정 방법이 아니기 때문)**

#### 오차 행렬

이진 분류 평가 결과를 나타낼 때 가장 널리 사용하는 방법 중 하나

```python
from sklearn.metrics import confusion_matrix

confusion = confusion_matrix(y_test, pred_logreg)
print("오차 행렬:\n", confusion)
# 오차 행렬:
# [[401   2]
# [  8  39]]
```

행은 정답 클래스에 해당하며 열은 예측 클래스에 해당 **(오차 행렬의 대각 행렬{(0, 0), (1, 1)}은 정확히 분류된 경우, 다른 항목은 한 클래스의 샘플들이 다른 클래스로 잘못 분류된 경우)**

<img src="https://user-images.githubusercontent.com/58063806/114295435-3788dc00-9ae0-11eb-9552-937122a37fda.png" width=50% />

**숫자 9를 양성 클래스로 정의**

<img src="https://user-images.githubusercontent.com/58063806/114295489-90587480-9ae0-11eb-9dc9-3736c762aecb.png" width=50% />

```python
print("빈도 기반 더미 모델:\n", confusion_matrix(y_test, pred_most_frequent))
print("\n무작위 더미 모델:\n", confusion_matrix(y_test, pred_dummy))
print("\n로지스틱 회귀:\n", confusion_matrix(y_test, pred_logreg))
# 빈도 기반 더미 모델:
# [[403   0]
# [ 47   0]]
# 무작위 더미 모델:
# [[355  48]
# [ 45   2]]
# 로지스틱 회귀:
# [[401   2]
# [  8  39]]
```

정확도 - 정확히 예측한 수를 전체 샘플 수로 나눈 값

정밀도 (양성 예측도, PPV) - 거짓 양성의 수를 줄이는 것이 목표일 때 성능 지표로 사용 **(TP / TP + FP, 양성으로 예측된 것 중 얼마나 많은 샘플이 진짜 양성인지 측정)**

EX) 임상 실험을 통해 신약의 치료 효과를 예측하는 모델

재현율 (민감도, 적중률, 진짜 양성 비율(TPR)) - 모든 양성 샘플을 식별해야 할 때 (거짓 음성을 피해야하는 것이 중요할 때) 성능 지표로 사용 **(TP / TP + FN, 전체 양성 샘플중 얼마나 많은 샘플이 양성 클래스로 분류되는지 측정)**

EX) 암을 미리 진단하는 모델



정밀도와 재현율의 최적화는 상충함

이 둘의 조화 평균인 f-score는 이 둘을 하나로 요약

f1-score = 2 * (정밀도 * 재현율 ) / (정밀도 + 재현율)

```python
from sklearn.metrics import f1_score

print("빈도 기반 더미 모델 f1 score: {:.2f}".format(f1_score(y_test, pred_most_frequent)))
print("무작위 더미 모델 f1 score: {:.2f}\n".format(f1_score(y_test, pred_dummy)))
print("로지스틱 회귀 f1 score: {:.2f}\n".format(f1_score(y_test, pred_logreg)))
# 빈도 기반 더미 모델 f1 score: 0.00
# 무작위 더미 모델 f1 score: 0.04
# 로지스틱 회귀 f1 score: 0.89
```

정확도에 비해 뚜렷한 차이를 보임

**어떤 모델이 좋은지 직관적으로 판단하는 데에는 정확도보다 f1-score가 나음**

```python
from sklearn.metrics import classification_report

print(classification_report(y_test, pred_most_frequent, target_names=["9 아님", "9"]))
print(classification_report(y_test, pred_dummy, target_names=["9 아님", "9"]))
print(classification_report(y_test, pred_logreg, target_names=["9 아님", "9"]))
```

<img src="https://user-images.githubusercontent.com/58063806/114296098-3c4f8f00-9ae4-11eb-942b-28848405b8a3.png" width=40% />

support - 각 클래스에 대한 지지도 (해당 클래스에 있는 진짜 샘플의 수)

마지막 세줄은 정밀도, 재현율, f1-score의 평균

macro avg - 단순히 클래스별 점수의 평균을 계산

weighted avg - 클래스의 샘플 수로 가중 평균

#### 불확실성 고려

음성 클래스 데이터 포인트 400개와 양성 클래스 데이터 포인트 50개로 이루어진 불균형 데이터셋

이진 탐색에서 decision_function은 0, predict_proba는 0.5를 임계값으로 사용

```python
from sklearn.datasets import make_blobs
from sklearn.svm import SVC

X, y = make_blobs(n_samples=(400, 50), cluster_std=[7.0, 2], random_state=22)
X_train, X_test, y_train, y_test = train_test_split(X, y , random_state=0)

svc = SVC(gamma=.05).fit(X_train, y_train)

print(classification_report(y_test, svc.predict(X_test)))
#               precision    recall  f1-score   support

#            0       0.97      0.89      0.93       104
#            1       0.35      0.67      0.46         9

#     accuracy                           0.88       113
#    macro avg       0.66      0.78      0.70       113
# weighted avg       0.92      0.88      0.89       113
```

소수 클래스인 1에 대해 상당히 작은 정밀도를 얻었으며 재현율도 0.67로 높지 않음

암 진단과 같이 클래스 1의 재현율을 높이는 것이 중요하다고 가정 (FP가 늘어나더라도 TP를 늘려야 함)

```python
y_pred_lower_threshold = svc.decision_function(X_test) > -.8

print(classification_report(y_test, y_pred_lower_threshold)) 
#               precision    recall  f1-score   support

#            0       1.00      0.82      0.90       104
#            1       0.32      1.00      0.49         9

#     accuracy                           0.83       113
#    macro avg       0.66      0.91      0.69       113
# weighted avg       0.95      0.83      0.87       113
```

기본적으로 decision_function의 값이 0보다 큰 포인트는 클래스 1로 분류되지만 더 많은 포인트를 클래스 1로 분류하기 위해 임계값을 낮춤

클래스 1의 재현율이 높아졌고 정밀도는 낮아짐 (넓은 영역이 클래스 1로 분류) 

```python
mglearn.plots.plot_decision_threshold()
```

<img src="https://user-images.githubusercontent.com/58063806/114302683-07076900-9b05-11eb-96f0-4488eff2ee59.png" width=90% />

실전에서는 테스트 세트를 이용해 임계값을 조정하면 안됨

#### 정밀도-재현율 곡선과 ROC 곡선

- 모델의 분류 작업을 결정하는 임계값은 비즈니스 목표에 따라 결정되며 해당 분류기의 정밀도와 재현율의 상충 관계를 조정하는 일
- 분류기의 필요조건을 지정하는 것을 **운영 포인트를** 지정한다고 지칭
- 한 번에 정밀도나 재현율의 모든 장단점을 살펴보기 위해 정밀도-재현율 곡선을 사용

```python
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(y_test, svc.decision_function(X_test))

X, y = make_blobs(n_samples=(4000, 500), cluster_std=[7.0, 2], random_state=22)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

svc = SVC(gamma=.05).fit(X_train, y_train)

precision, recall, thresholds = precision_recall_curve(y_test, svc.decision_function(X_test))
# 0에 가까운 임계값
close_zero = np.argmin(np.abs(thresholds))

plt.rc('font', family="Malgun Gothic")
plt.plot(precision[close_zero], recall[close_zero], "o", markersize=10, label="임계값 0", fillstyle="none", c="k", mew=2)

plt.plot(precision, recall, label="정밀도-재현율 곡선")
plt.xlabel("정밀도")
plt.ylabel("재현율")
plt.legend(loc="best")
```

<img src="https://user-images.githubusercontent.com/58063806/114304048-150cb800-9b0c-11eb-91e5-8279e427e184.png" width=50% />

- 임계값 0 지점은 predict 메서드를 호출할 때 사용되는 임계값
- 임계값이 커지면서 곡선은 정밀도가 높아지는 쪽으로 이동 (재현율은 낮아짐)
  - 임계값을 높일수록 양성으로 분류된 포인트 대부분이 진짜 양성(TP)이 됨
- **곡선이 오른쪽 위로 갈수록 더 좋은 분류기 (오른쪽 위 지점은 한 임계값에서 정밀도와 재현율이 모두 높음을 의미)**
- 분류기가 다르면 곡선의 다른 부분에서 장점이 생김 (운영 포인트가 달라짐)

```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)
rf.fit(X_train, y_train)

# randomforest는 decision_function 제공 X
# 정밀도-재현율 함수는 양성 클래스의 확신에 대한 측정값을 두 번째 매개변수로 받음
# 샘플이 클래스 1일 확률을 인수로 넘김
precision_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test, rf.predict_proba(X_test)[:, 1])

plt.plot(precision, recall, label="svc")
plt.plot(precision[close_zero], recall[close_zero], "o", markersize=10, label="svc: 임계값 0", fillstyle="none", c="k", mew=2)
plt.plot(precision_rf, recall_rf, label="rf")

close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))
plt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], "^", label="rf: 임계값 0.5", c="k", markersize=10, fillstyle="none", mew=2)
plt.xlabel("정밀도")
plt.ylabel("재현율")
plt.legend(loc="best")
```

<img src="https://user-images.githubusercontent.com/58063806/114304532-a3823900-9b0e-11eb-8167-079c0da535d6.png" width=50% />

- 재현율이 매우 높거나 정밀도가 매우 높을 때는 랜덤 포레스트가 더 낫고 가운데 근처에서는 svc가 더 나은 결과를 볼 수 있음

```python
print("랜덤 포레스트의 f1_score: {:.2f}".format(f1_score(y_test, rf.predict(X_test))))
print("SVC의 f1_score: {:.2f}".format(f1_score(y_test, svc.predict(X_test))))
# 랜덤 포레스트의 f1_score: 0.61
# SVC의 f1_score: 0.66
```

기본 임계값에 대한 f1-score를 보면 역시 svc가 더 나은 점수를 보임

전체 곡선에 담긴 정보를 요약하는 방법으로 **정밀도-재현율 곡선의 아랫부분 면적**을 계산할 수 있으며, 이를 **평균 정밀도**라고 함

```python
from sklearn.metrics import average_precision_score

ap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])
ap_svc = average_precision_score(y_test, svc.decision_function(X_test))
print("랜덤 포레스트의 평균 정밀도: {:.3f}".format(ap_rf))
print("SVC의 평균 정밀도: {:.3f}".format(ap_svc))
# 랜덤 포레스트의 평균 정밀도: 0.660
# SVC의 평균 정밀도: 0.666
```

svc가 조금 더 높지만, 거의 같은 성능

**평균 정밀도는 0 ~ 1 사이를 지나는 곡선의 아래 면적이므로 항상 0 (가장 나쁨)과 1 (가장 좋음) 사이의 값을 반환)**

#### ROC & AUC

- 정밀도-재현율 곡선과 비슷하게 ROC 곡선은 모든 임계값을 고려하지만, 정밀도와 재현율 대신 **진짜 양성 비율(TPR)**에 대한 **거짓 양성 비율(FPR)**을 나타냄
- 진짜 양성 비율은 재현율, 거짓 양성 비율은 전체 음성 샘플 중 거짓 양성으로 잘못 분류한 비율을 나타냄 

```python
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))

plt.plot(fpr, tpr, label="ROC 곡선")
plt.xlabel("FPR")
plt.ylabel("TPR (재현율)")
close_zero = np.argmin(np.abs(thresholds))
plt.plot(fpr[close_zero], tpr[close_zero], "o", markersize=10, label="임계값 0", fillstyle="none", c="k", mew=2)
plt.legend(loc="best")
```

<img src="https://user-images.githubusercontent.com/58063806/114305191-fe695f80-9b11-11eb-97ce-92f984e00bb6.png" width=60% />

ROC 곡선은 왼쪽 위에 가까울수록 이상적 (재현율은 높으면서 거짓 양성 비율은 낮게 유지)

```python
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])

plt.plot(fpr, tpr, label="SVC의 ROC 곡선")
plt.plot(fpr_rf, tpr_rf, label="RF의 ROC 곡선")

plt.xlabel("FPR")
plt.ylabel("TPR (재현율)")
plt.plot(fpr[close_zero], tpr[close_zero], "o", markersize=10, label="SVC 임계값 0", fillstyle="none", c="k", mew=2)

close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))
plt.plot(fpr_rf[close_default_rf], tpr_rf[close_default_rf], "^", markersize=10, label="RF 임계값 0.5", fillstyle="none", c="k", mew=2)
plt.legend(loc="best")
```

<img src="https://user-images.githubusercontent.com/58063806/114305683-41c4cd80-9b14-11eb-950a-e7c38e2a8d81.png" width=60% />

정밀도-재현율 곡선과 마찬가지로 곡선 아래의 **면적값 하나로 ROC 곡선을 요약할 수 있는데 이를 AUC (area under the curve)**라고 함

```python
from sklearn.metrics import roc_auc_score

rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])
svc_auc = roc_auc_score(y_test, svc.decision_function(X_test))
print("랜덤 포레스트의 AUC: {:.3f}".format(rf_auc))
print("SVC의 AUC: {:.3f}".format(svc_auc))
# 랜덤 포레스트의 AUC: 0.937
# SVC의 AUC: 0.916
```

랜덤 포레스트의 성능이 SVC보다 조금 더 나은 것을 볼 수 있음

- AUC도 마찬가지로 0 ~ 1 사이의 곡선 아래 면적이므로 항상 0 (최악)과 1 (최선) 사이의 값을 가짐
- 데이터셋에 담긴 클래스가 아무리 불균형해도 무작위로 예측한 AUC 값은 0.5가 됨
- 불균형한 데이터셋에서는 정확도보다 AUC가 훨씬 좋은 지표
- AUC는 양성 샘플의 순위를 평가하는 것으로 볼 수 있음 (분류기에서 무작위로 선택한 양성 클래스 포인트의 점수가 무작위로 선택한 음성 클래스 포인트의 점수보다 높을 확률과 같음)
  - AUC가 1일 때는 모든 양성 포인트의 점수가 모든 음성 포인트의 점수보다 높음

```python
y = digits.target == 9

X_train, X_test, y_train, y_test = train_test_split(digits.data, y, random_state=0)

plt.figure()

for gamma in [1, 0.1, 0.01]:
    svc = SVC(gamma=gamma).fit(X_train, y_train)
    accuracy = svc.score(X_test, y_test)
    auc = roc_auc_score(y_test, svc.decision_function(X_test))
    fpr, tpr, _ = roc_curve(y_test, svc.decision_function(X_test))
    print("gamma = {:.2f} 정확도 = {:.2f} AUC = {:.2f}".format(gamma, accuracy, auc))
    plt.plot(fpr, tpr, label="gamma={:.2f}".format(gamma))
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.xlim(-0.01, 1)
plt.ylim(0, 1.02)
plt.legend(loc="best")

# gamma = 1.00 정확도 = 0.90 AUC = 0.50
# gamma = 0.10 정확도 = 0.90 AUC = 0.96
# gamma = 0.01 정확도 = 0.90 AUC = 1.00
```

<img src="https://user-images.githubusercontent.com/58063806/114306261-5a35e780-9b16-11eb-834d-4d59b33580b3.png" width=60% />

gamma=1.0에서는 AUC는 무작위로 선택한 수준

gamma=0.1에서는 AUC 값이 0.96으로 크게 향상

gamma=0.01에서는 완벽한 AUC 값 1을 얻음 (적절한 임계값에서 이 모델은 데이터를 완벽하게 분류할 수 있음) 

**불균형 데이터셋에서 모델을 평가하 때는 AUC 사용을 강력히 권장**

**(AUC가 높은 모델에서 좋은 분류 결과를 얻으려면 결정 임계값을 조정해야 함)**



#### 다중 분류의 평가 지표

- 다중 분류를 위한 지표는 모두 이진 분류 평가 지표에서 유도
- 모든 클래스에 대해 평균을 낸 것
- 다중 분류의 정확도도 분류된 샘플의 비율로 정의하기 때문에 클래스가 불균형한 경우에는 좋은 평가 방법이 아님

```python
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=0)

lr = LogisticRegression(solver="liblinear", multi_class="ovr").fit(X_train, y_train)
pred = lr.predict(X_test)
print("정확도: {:.3f}".format(accuracy_score(y_test, pred)))
print("오차 행렬:\n", confusion_matrix(y_test, pred))
# 정확도: 0.953
# 오차 행렬:
# [[37  0  0  0  0  0  0  0  0  0]
# [ 0 39  0  0  0  0  2  0  2  0]
# [ 0  0 41  3  0  0  0  0  0  0]
# [ 0  0  1 43  0  0  0  0  0  1]
# [ 0  0  0  0 38  0  0  0  0  0]
# [ 0  1  0  0  0 47  0  0  0  0]
# [ 0  0  0  0  0  0 52  0  0  0]
# [ 0  1  0  1  1  0  0 45  0  0]
# [ 0  3  1  0  0  0  0  0 43  1]
# [ 0  0  0  1  0  1  0  0  1 44]]
```

이진 분류와 마찬가지로 각 행은 정답 레이블, 열은 예측 레이블에 해당

```python
import mglearn
import matplotlib.pyplot as plt

plt.rc("font", family="Malgun Gothic")
scores_image = mglearn.tools.heatmap(confusion_matrix(y_test, pred), xlabel="예측 레이블", ylabel="진짜 레이블", 
                                     xticklabels=digits.target_names, yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt="%d")

plt.title("오차 행렬")
plt.gca().invert_yaxis()
```

<img src="https://user-images.githubusercontent.com/58063806/114405705-e7923e00-9be1-11eb-8c17-dc3ea09d69b8.png" width=40% />

```python
from sklearn.metrics import classification_report

print(classification_report(y_test, pred))
```

<img src="https://user-images.githubusercontent.com/58063806/114405685-e234f380-9be1-11eb-9458-168e9315bc35.png" width=40% />

행 방향으로는 precision (모델이 해당 레이블로 예측한 데이터의 개수)

열 방향으로 recall (실제로 해당 레이블 값을 가지는 데이터의 개수)

해당 모델은 클래스 1, 3, 8을 분류하는데 어려움을 겪음

- 다중 분류에서 불균형 데이터셋에 가장 널리 사용되는 평가 지표는 **f1-score의 다중 분류 버전**
  - 한 클래스를 양성 클래스로 두고 나머지 클래스들을 음성 클래스로 간주하여 클래스마다 f1-score를 계산
  - 그런 다음, 클래스별 f1-score를 다음 전략 중 하나를 사용하여 평균

> "macro" 평균 - 클래스별 f1-score에 가중치를 주지 않음 (클래스 크기에 상관없이 모든 클래스를 동등하게 취급)
>
> "weighted" 평균 - 클래스별 샘플 수로 가중치를 두어 f1-score의 평균을 계산
>
> "micro" 평균 - 모든 클래스의 FP, FN, TP의 총 수를 헤아린 다음 정밀도, 재현율, F1-score를 이 수치로 계산

```python
from sklearn.metrics import f1_score

print("micro 평균 f1 score: {:.3f}".format(f1_score(y_test, pred, average="micro")))
print("macro 평균 f1 score: {:.3f}".format(f1_score(y_test, pred, average="macro")))
# micro 평균 f1 score: 0.953
# macro 평균 f1 score: 0.954
```

**각 샘플을 똑같이 간주하면 "micro", 각 클래스를 동일한 비중으로 고려한다면 "macro" 평균** 

#### 회귀의 평가 지표

- **대부분의 애플리케이션에서는 회귀 추정기의 score 메서드에서 이용하는 R^2만으로 충분**
- 평균 제곱 에러나 평균 절댓값 에러를 이용해서 모델을 튜닝할 때 이런 지표를 기반으로 비즈니스 결정을 할 수 있음

#### 모델 선택에서 평가 지표 사용

```python
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC

print("기본 평가 지표:", cross_val_score(SVC(), digits.data, digits.target == 9, cv=5))
# 기본 평가 지표: [0.9       0.9       0.89972145 0.89972145 0.89972145]

explicit_accuracy = cross_val_score(SVC(), digits.data, digits.target == 9, scoring="accuracy", cv=5)
print("정확도 지표:", explicit_accuracy)
# 정확도 지표: [0.9       0.9       0.89972145 0.89972145 0.89972145]

roc_auc = cross_val_score(SVC(), digits.data, digits.target == 9, scoring="roc_auc", cv=5)
print("평균 정밀도 지표:", roc_auc)
# 평균 정밀도 지표: [0.99674211 0.99725652 0.99552804 0.99836601 0.99165807]
```

```python
import pandas as pd
from sklearn.model_selection import cross_validate

res = cross_validate(SVC(), digits.data, digits.target == 9, scoring=["accuracy", "roc_auc", "recall_macro"], 
                     return_train_score=True, cv=5)
display(pd.DataFrame(res))
```

<img src="https://user-images.githubusercontent.com/58063806/114410229-2aeeab80-9be6-11eb-9c7a-64cadd9f2175.png" width=100% />

cross_validate 함수를 이용해 한 번에 여러 측정 지표를 계산

```python
from sklearn.metrics import average_precision_score
from sklearn.model_selection import GridSearchCV

X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target == 9, random_state=0)

param_grid = {"gamma": [0.0001, 0.01, 0.1, 1, 10]}
grid = GridSearchCV(SVC(), param_grid=param_grid, cv=3)
grid.fit(X_train, y_train)

print("정확도 지표를 사용한 그리드 서치")
print("최적의 파라미터:", grid.best_params_)
print("최상의 교차 검증 점수(정확도): {:.3f}".format(grid.best_score_))
print("테스트 세트 평균 정밀도: {:.3f}".format(average_precision_score(y_test, grid.decision_function(X_test))))
print("테스트 세트 정확도: {:.3f}".format(grid.score(X_test, y_test)))

# 정확도 지표를 사용한 그리드 서치
# 최적의 파라미터: {'gamma': 0.0001}
# 최상의 교차 검증 점수(정확도): 0.970
# 테스트 세트 평균 정밀도: 0.966
# 테스트 세트 정확도: 0.973

grid = GridSearchCV(SVC(), param_grid=param_grid, scoring="average_precision", cv=3)
grid.fit(X_train, y_train)
print("평균 정밀도 지표를 사용한 그리드 서치")
print("최적의 파라미터:", grid.best_params_)
print("최상의 교차 검증 점수(평균 정밀도): {:.3f}".format(grid.best_score_))
print("테스트 세트 평균 정밀도: {:.3f}".format(average_precision_score(y_test, grid.decision_function(X_test))))
print("테스트 세트 정확도: {:.3f}".format(accuracy_score(y_test, grid.best_estimator_.predict(X_test))))

# 평균 정밀도 지표를 사용한 그리드 서치
# 최적의 파라미터: {'gamma': 0.01}
# 최상의 교차 검증 점수(평균 정밀도): 0.985
# 테스트 세트 평균 정밀도: 0.996
# 테스트 세트 정확도: 0.896
```

평가 지표에 따라 최적의 파라미터는 물론 score도 달라짐

```python
from sklearn.metrics.scorer import SCORERS

print(SCORERS.keys())
```

가능한 평가 방식을 모두 볼 수 있음