## 군집 알고리즘 비교와 평가

#### 타깃값으로 군집 평가

- 군집 알고리즘의 결과를 실제 정답 클러스터와 비교해서 평가
  - 1(최적일 때), 0(무작위로 분류될 때) 사이의 값을 제공하는 ARI와 NMI (ARI는 음수가 될 수 있음)

```python
import matplotlib.pyplot as plt
import numpy as np
import mglearn
from sklearn.metrics.cluster import adjusted_rand_score
from sklearn.datasets import make_moons
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN

X, y = make_moons(n_samples=200, noise=0.05, random_state=0)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

fig, axes = plt.subplots(1, 4, figsize=(15, 3), subplot_kw={'xticks':  (), 'yticks': ()})

algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2), DBSCAN()]

# 무작위 클러스터
random_clusters = np.random.RandomState(seed=0).randint(0, 2, len(X))

plt.rc('font', family="Malgun Gothic")
axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters, cmap=mglearn.cm3, s=60, edgecolors="black")
axes[0].set_title("무작위 할당 - ARI: {:.2f}".format(adjusted_rand_score(y, random_clusters)))

for ax, algorithm in zip(axes[1:], algorithms):
    clusters = algorithm.fit_predict(X_scaled)
    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm3, s=60, edgecolors="black")
    ax.set_title("{} - ARI: {:.2f}".format(algorithm.__class__.__name__, adjusted_rand_score(y, clusters)))
```

<img src="https://user-images.githubusercontent.com/58063806/112324640-5a507d80-8cf6-11eb-87be-36044731f740.png" width=100% />

#### 타깃값 없이 군집 평가

- 군집 알고리즘을 적용할 때 보통은 그 결과와 비교할 타깃값이 없음
  - 그러므로 ARI, NMI 같은 지표는 성능 평가가 아니라 알고리즘을 개발할때나 도움
- 타깃값이 필요 없는 군집용 지표로는 **실루엣 계수**가 있음

> **실루엣 점수는 클러스터의 밀집 정도를 계산하는 것**으로 높을수록 좋으며 1이 최댓값 (하지만 모양이 복합할 때는 밀집도를 활용한 평가가 잘 동작하지 않음)

```python
import matplotlib.pyplot as plt
import numpy as np
import mglearn
from sklearn.metrics.cluster import silhouette_score
from sklearn.datasets import make_moons
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN

X, y = make_moons(n_samples=200, noise=0.05, random_state=0)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

fig, axes = plt.subplots(1, 4, figsize=(15, 3), subplot_kw={'xticks':  (), 'yticks': ()})

algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2), DBSCAN()]

# 무작위 클러스터
random_clusters = np.random.RandomState(seed=0).randint(0, 2, len(X))

plt.rc('font', family="Malgun Gothic")
axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters, cmap=mglearn.cm3, s=60, edgecolors="black")
axes[0].set_title("무작위 할당 : {:.2f}".format(silhouette_score(X_scaled, random_clusters)))

for ax, algorithm in zip(axes[1:], algorithms):
    clusters = algorithm.fit_predict(X_scaled)
    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm3, s=60, edgecolors="black")
    ax.set_title("{} : {:.2f}".format(algorithm.__class__.__name__, silhouette_score(X_scaled, clusters)))
```

<img src="https://user-images.githubusercontent.com/58063806/112326275-bbc51c00-8cf7-11eb-9bf1-bca678a4f0b2.png" width=100% />

실제로 DBSCAN의 결과가 가장 좋지만 점수는 낮은 것을 볼  수 있음

- 클러스터 평가에 더 적합한 전략은 견고성 기반의 지표
- 데이터에 잡음 포인트를 추가하거나 여러 가지 매개변수 설정으로 알고리즘을 실행하고 그 결과를 비교했을 때 결과가 일정하다면 신뢰할만한 결과로 볼 수 있음

#### 얼굴 데이터셋을 이용한 군집 평가

##### DBSCAN

```python
from sklearn.decomposition import PCA
from sklearn.datasets import fetch_lfw_people
from sklearn.cluster import DBSCAN
import numpy as np

people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)
image_shape = people.images[0].shape

mask = np.zeros(people.target.shape, dtype=np.bool)
for target in np.unique(people.target):
    mask[np.where(people.target == target)[0][:50]] = 1
X_people = people.data[mask]
y_people = people.target[mask]
X_people = X_people / 255.

pca = PCA(n_components=100, whiten=True, random_state=0)
X_pca = pca.fit_transform(X_people)

dbscan = DBSCAN()
labels = dbscan.fit_predict(X_pca)
print("고유한 레이블:", np.unique(labels))
# 고유한 레이블: [-1], 잡음 포인트로 레이블 된 것을 볼 수 있음
```

**min_samples, eps 조정**

```python
dbscan = DBSCAN(min_samples=3, eps=15)
labels = dbscan.fit_predict(X_pca)
print("고유한 레이블:", np.unique(labels))
# 고유한 레이블: [-1  0]

# 각 레이블에 속한 포인트의 수를 셈
# bincount는 음수를 받을 수 없어서 + 1
print("클러스터별 포인트 수:", np.bincount(labels + 1))
# 클러스터별 포인트 수: [  32 2031]
```

**잡음 포인트로 레이블된 데이터**

```python
import matplotlib.pyplot as plt

noise = X_people[labels==-1]

fig, axes = plt.subplots(3, 9, subplot_kw={'xticks': (), 'yticks': ()}, figsize=(12, 4))
for image, ax in zip(noise, axes.ravel()):
    ax.imshow(image.reshape(image_shape), vmin=0, vmax=1, cmap="gray")
```

<img src="https://user-images.githubusercontent.com/58063806/112482114-0d82aa80-8dbb-11eb-84dc-8625eabef3c1.png" width=100% />

손이나 컵으로 얼굴을 가리고, 모자를 쓰고, 얼굴 각도가 이상하거나 너무 가까이 혹은 너무 멀리 자른 경우들을 볼 수 있음 **(이상치 검출)** 

```python
for eps in [1, 3, 5, 7, 9, 11, 13]:
    print("\neps=", eps)
    dbscan = DBSCAN(min_samples=3, eps=eps)
    labels= dbscan.fit_predict(X_pca)
    print("클러스터 수:", len(np.unique(labels)))
    print("클러스터별 포인트 수:", np.bincount(labels + 1))
# eps= 1
# 클러스터 수: 1
# 클러스터별 포인트 수: [2063]

# eps= 3
# 클러스터 수: 1
# 클러스터별 포인트 수: [2063]

# eps= 5
# 클러스터 수: 1
# 클러스터별 포인트 수: [2063]

# eps= 7
# 클러스터 수: 14
# 클러스터별 포인트 수: [2004    3   14    7    4    3    3    4    4    3    3    5    3    3]

# eps= 9
# 클러스터 수: 4
# 클러스터별 포인트 수: [1307  750    3    3]

# eps= 11
# 클러스터 수: 2
# 클러스터별 포인트 수: [ 413 1650]

# eps= 13
# 클러스터 수: 2
# 클러스터별 포인트 수: [ 120 1943]
```

eps가 작으면 모든 포인트가 잡음으로 레이블, eps=7, 9에서 잡음 포인트가 많지만 다른 작은 클러스터들을 얻음 (큰 클러스터는 하나고 나머지는 작은 클러스터들, 모든 이미지는 거의 동일하게 나머지 이미지들과 유사)

**eps=7일때 얻은 클러스터들의 포인트를 시각화**

```python
dbscan = DBSCAN(min_samples=3, eps=7)
labels= dbscan.fit_predict(X_pca)

for cluster in range(max(labels) + 1):
    mask = labels == cluster
    n_images = np.sum(mask)
    fig, axes = plt.subplots(1, 14, figsize=(14 * 1.5, 4), subplot_kw={'xticks': (), 'yticks': ()})
    i = 0
    for image, label, ax in zip(X_people[mask], y_people[mask], axes):
        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1, cmap="gray")
        ax.set_title(people.target_names[label].split()[-1])
        i += 1
    for j in range(len(axes) - i):
        axes[j + i].imshow(np.array([[1]*65]*87), vmin=0, vmax=1, cmap="gray")
        axes[j + i].axis('off')
```

<img src="https://user-images.githubusercontent.com/58063806/112484381-3a37c180-8dbd-11eb-88ac-606a7c875318.png" width=80% />

같은 클러스터의 이미지들은 얼굴 표정, 각도가 매우 유사

##### KMeans

```python
from sklearn.cluster import KMeans

km = KMeans(n_clusters=10, random_state=0)
labels_km = km.fit_predict(X_pca)
print("클러스터별 포인트 수:", np.bincount(labels_km))
# 클러스터별 포인트 수: [155 175 238  75 358 257  91 219 323 172]
```

데이터 포인트들을 비교적 비슷한 크기로 클러스터링

**KMeans cluster 중심**

```python
fig, axes = plt.subplots(2, 5, figsize=(12, 4), subplot_kw={'xticks': (), 'yticks': ()})

for center, ax in zip(km.cluster_centers_, axes.ravel()):
    ax.imshow(pca.inverse_transform(center).reshape(image_shape), vmin=0, vmax=1, cmap="gray")
```

<img src="https://user-images.githubusercontent.com/58063806/112485618-7881b080-8dbe-11eb-8507-738ae6f96812.png" width=80% />

중심 이미지는 각 클러스터 데이터 포인트들의 평균이기 때문에 매우 부드럽게 나타남 (PCA 차원이 감소된 이유도 있음)

**클러스터 중심 이미지에 가장 가까운 이미지 5개와 가장 다른 이미지 5개**

```python
import mglearn

mglearn.plots.plot_kmeans_faces(km, pca, X_pca, X_people, y_people, people.target_names)
plt.gray()
plt.show()
```

<img src="https://user-images.githubusercontent.com/58063806/112486530-4cb2fa80-8dbf-11eb-8262-8ce4abf857da.png" width=70% />

세 번째 클러스터는 웃는 얼굴이고 나머지 클러스터들은 얼굴 각도를 중시함

중심에서 먼 포인트들은 클러스터 중심과 많이 다르고 특별한 규칙이 없는 것 같이 보임 (KMeans는 모든 포인트를 구분하기 때문)

##### AgglomerativeClustering

```python
from sklearn.cluster import AgglomerativeClustering

agglomerative = AgglomerativeClustering(n_clusters=10)
labels_agg = agglomerative.fit_predict(X_pca)
print("클러스터별 포인트 수:", np.bincount(labels_agg))
# 클러스터별 포인트 수: [169 660 144 329 217  85  18 261  31 149]
```

k-means 보다는 아니지만 dbscan에 비하면 비교적 고른 분포를 보임

```python
from sklearn.metrics import adjusted_rand_score

print("ARI: {:.2f}".format(adjusted_rand_score(labels_agg, labels_km)))
# ARI: 0.09
```

k-means로 생성한 클러스터와는 거의 공통 부분이 없는 것을 볼 수 있음 

```python
from scipy.cluster.hierarchy import dendrogram, ward

linkage_array = ward(X_pca)

plt.rc('font', family="Malgun Gothic")
plt.figure(figsize=(20, 5))
dendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)
plt.xlabel("샘플 번호")
plt.ylabel("클러스터 거리")
ax = plt.gca()
bounds = ax.get_xbound()
ax.plot(bounds, [36, 36], "--", c="k")
```

<img src="https://user-images.githubusercontent.com/58063806/112488274-bbdd1e80-8dc0-11eb-995b-4b40f779cd04.png" width=100% />

위의 결과를 보고는 가지의 길이를 보고 데이터를 나누고 있는 클러스터를 확인하기 어려움

```python
n_clusters=10
for cluster in range(n_clusters):
    mask = labels_agg == cluster
    fig, axes = plt.subplots(1, 10, figsize=(15, 8), subplot_kw={'xticks': (), 'yticks': ()})
    axes[0].set_ylabel(np.sum(mask))
    for image, label, asdf, ax in zip(X_people[mask], y_people[mask], labels_agg[mask], axes):
        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1, cmap="gray")
        ax.set_title(people.target_names[label].split()[-1], fontdict={'fontsize': 9})
```

<img src="https://user-images.githubusercontent.com/58063806/112489573-e8de0100-8dc1-11eb-94ad-cf40a083f615.png" width=100%/>

일부 클러스터는 일관성이 있지만 일관성을 가지기에는 너무 큰 클러스터들도 존재

클러스터의 개수를 늘리면 더욱 일관성을 가지는 클러스터들을 얻는 것을 볼 수 있음

#### 알고리즘별 특징

**KMeans** 

- 원하는 클러스터의 개수를 지정
- 클러스터 중심을 사용해서 클러스터를 구분 (분해 방법으로 볼 수도 있음)

**AgglomerativeClustering** 

- 원하는 클러스터의 개수를 지정
- 전체 데이터의 분할 계층도를 만듬 (덴드로그램으로 확인 가능)

**DBSCAN** 

- 클러스터에 할당되지 않는 잡음 포인트를 인식
- 클러스터의 개수를 자동으로 결정
- 복잡한 클러스터 모양을 인식가능
- 크기가 많이 다른 클러스터를 만들어냄